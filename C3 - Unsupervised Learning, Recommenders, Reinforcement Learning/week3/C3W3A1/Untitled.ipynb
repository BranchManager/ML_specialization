{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4257cde4-9c87-48f9-bddc-3f1ab2bfcc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "#import gym\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "import random\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import public_tests\n",
    "#from tensorflow.keras import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Input\n",
    "#from tensorflow.keras.losses import MSE\n",
    "#from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "26e56790-4bdd-4cbc-a6ec-c67e4e06e867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f166c730990>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Display(visible=0, size=(840, 480)).start();\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "906efc0a-9de1-4081-85ee-67cf39017726",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "723f6ade-73d4-4622-8e0a-62069c0836d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7d6c7e2f-c564-4b5c-a471-8ef7bdee9dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Shape: (8,)\n",
      "Number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "\n",
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print('State Shape:', state_size)\n",
    "print('Number of actions:', num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1a2af14c-d74f-4f60-917a-95272c33da94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tuple\n",
      "<class 'tuple'>\n",
      "Initial State: (array([-0.004, 1.419, -0.435, 0.365, 0.005, 0.099, 0.000, 0.000],\n",
      "      dtype=float32), {})\n",
      "Action: 0\n",
      "Next State: [-0.009 1.427 -0.435 0.339 0.010 0.097 0.000 0.000]\n",
      "Reward Received: 0.41100255639230454\n",
      "Episode Terminated: False\n",
      "Info: False\n",
      "F : {}\n"
     ]
    }
   ],
   "source": [
    "# Select an action\n",
    "action = 0\n",
    "\n",
    "initial_state = env.reset()\n",
    "\n",
    "# Run a single time step of the environment's dynamics with the given action.\n",
    "next_state, reward, done, info, f = env.step(action)\n",
    "#tuple = env.step(action)\n",
    "print(\"The tuple\")\n",
    "print(tuple)\n",
    "\n",
    "with np.printoptions(formatter={'float': '{:.3f}'.format}):\n",
    "    print(\"Initial State:\", initial_state)\n",
    "    print(\"Action:\", action)\n",
    "    print(\"Next State:\", next_state)\n",
    "    print(\"Reward Received:\", reward)\n",
    "    print(\"Episode Terminated:\", done)\n",
    "    print(\"Info:\", info)\n",
    "    print(\"F :\",f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a0d6a291-048c-4569-af6a-b6c870821053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNQ_C1\n",
    "# # GRADED CELL\n",
    "\n",
    "# # Create the Q-Network\n",
    "# print(\"State size :\",type(state_size))\n",
    "# q_network = Sequential([\n",
    "#     ### START CODE HERE ### \n",
    "#     Input(shape=state_size),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=num_actions,activation='linear'),\n",
    "    \n",
    "#     ### END CODE HERE ### \n",
    "#     ])\n",
    "\n",
    "# # Create the target Q^-Network\n",
    "# target_q_network = Sequential([\n",
    "#     ### START CODE HERE ### \n",
    "#     Input(shape=state_size),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=num_actions,activation='linear'),\n",
    "#     ### END CODE HERE ###\n",
    "#     ])\n",
    "\n",
    "# ### START CODE HERE ### \n",
    "# optimizer = Adam(learning_rate=ALPHA)\n",
    "# ### END CODE HERE ###\n",
    "\n",
    "# print(q_network.summary(expand_nested=True))\n",
    "\n",
    "# print(target_q_network.summary())\n",
    "\n",
    "# this is how you'd presumably create a dense nn on torch using the same\n",
    "# layout as tf\n",
    "q_network = nn.Sequential(\n",
    "    nn.Linear(state_size[0], 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 4),\n",
    "    \n",
    ")\n",
    "\n",
    "target_q_network = nn.Sequential(\n",
    "    nn.Linear(state_size[0], 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 4),\n",
    "    \n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(q_network.parameters(), lr=ALPHA)\n",
    "optimizer = torch.optim.Adam(target_q_network.parameters(), lr=ALPHA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "de3ca19b-c8c0-4894-ae21-9b3b15470367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 2. 1. 2. 0. 2. 3. 3. 2. 3. 3. 2. 0. 0. 0. 1. 1. 3. 0. 3. 3. 3. 2.\n",
      " 2. 1. 0. 1. 1. 2. 1. 1. 0. 0. 3. 1. 3. 0. 1. 2. 3. 1. 0. 1. 2. 2. 3. 0.\n",
      " 0. 2. 3. 0. 2. 2. 1. 3. 2. 0. 1. 3. 3. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "np.random.seed(1)\n",
    "states = np.float32(np.random.rand(64, 8))\n",
    "actions = np.float32(np.floor(np.random.uniform(0, 1, (64, )) * 4))\n",
    "rewards = np.float32(np.random.rand(64, ))\n",
    "next_states = np.float32(np.random.rand(64, 8))\n",
    "done_vals = np.float32((np.random.uniform(0, 1, size=(64,)) > 0.96) * 1)\n",
    "\n",
    "print(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c4b5f073-ffce-43d9-88db-6336f277ebb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "#     \"\"\" \n",
    "#     Calculates the loss.\n",
    "    \n",
    "#     Args:\n",
    "#       experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "#       gamma: (float) The discount factor.\n",
    "#       q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "#       target_q_network: (tf.keras.Sequential) Karas model for predicting the targets\n",
    "          \n",
    "#     Returns:\n",
    "#       loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "#             the y targets and the Q(s,a) values.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Unpack the mini-batch of experience tuples\n",
    "#     states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "#     # Compute max Q^(s,a)\n",
    "#     max_qsa = tf.reduce_max(target_q_network(next_states), axis=0)\n",
    "\n",
    "#     print(type(target_q_network(next_states)))\n",
    "    \n",
    "#     print(target_q_network(next_states))\n",
    "#     print(\"numpy shape \",target_q_network(next_states).shape)\n",
    "#     print(\"numpy max, \" ,np.max(target_q_network(next_states),axis=0))\n",
    "#     print(\"numpy dtype , \",target_q_network(next_states).dtype)\n",
    "#     print(max_qsa)\n",
    "    \n",
    "#     # Set y = R if episode terminates, otherwise set y = R + Î³ max Q^(s,a).\n",
    "#     ### START CODE HERE ### \n",
    "#     y = (rewards + (1-done_vals)*(gamma*max_qsa))\n",
    "#     ### END CODE HERE ###\n",
    "    \n",
    "#     # Get the q_values\n",
    "#     q_values = q_network(states)\n",
    "#     q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "#                                                 tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "#     # Compute the loss\n",
    "#     ### START CODE HERE ### \n",
    "#     loss = MSE(y, q_values)\n",
    "#     ### END CODE HERE ### \n",
    "    \n",
    "#     return loss\n",
    "\n",
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \n",
    "    # Unpack the mini-batch of experience tuples\n",
    "  \n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    #input_tensor = torch.randn(20,20)\n",
    "    #print(type(input_tensor))\n",
    "    #print(input_tensor)\n",
    "    print(type(states))\n",
    "    print(type(rewards))\n",
    "    #print(\"END\")\n",
    "    n_statae = target_q_network(next_states)\n",
    "\n",
    "    #print(n_statae)\n",
    "    max_qsa , indicies  = torch.max(torch.from_numpy(n_statae), dim=1)\n",
    "    \n",
    "    #print(\"Actions :\",actions)\n",
    "\n",
    "    #print(\"max_qsa \", max_qsa)\n",
    "    y = (rewards + (1-done_vals)*(gamma*max_qsa.numpy()))\n",
    "    \n",
    "    q_values = q_network(states)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #print(\"q_values ,\",q_values)\n",
    "    #print(\"action shape \",type(actions))\n",
    "    #print(np.arange(actions.shape[0]))\n",
    "    #print(\"q_values 64, \",q_values[np.arange(actions.shape[0]),actions.astype(int)])\n",
    "    q_values = q_values[np.arange(actions.shape[0]),actions.astype(int)]\n",
    "    #print(\"Y ,\",y.shape)\n",
    "    #print(\"q_values, \",q_values.size())\n",
    "    mseloss = torch.nn.MSELoss()\n",
    "    loss = mseloss(torch.from_numpy(y),torch.from_numpy(q_values))\n",
    "    #mse_loss = torch.nn.functional.mse_loss(y,q_values)\n",
    "\n",
    "    #print(loss)\n",
    "    return loss\n",
    "\n",
    "public_tests.test_compute_loss(compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b83f62bd-1b5d-4299-b493-0e7c88615e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "25bda1ea-aa62-45ee-ab55-d7ae1d30b834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0219,  0.1630,  0.1103, -0.0180], grad_fn=<ViewBackward0>)\n",
      "(array([-0.01112967,  1.4000072 , -0.5691295 , -0.25517553,  0.01476436,\n",
      "        0.16717382,  0.        ,  0.        ], dtype=float32), -2.1840408573874286, False, False, {})\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m new_states \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(new_action\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_states)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mval\u001b[49m)        \n",
      "\u001b[0;31mNameError\u001b[0m: name 'val' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "SEED = 0              # seed for pseudo-random number generator\n",
    "MINIBATCH_SIZE = 64   # mini-batch size\n",
    "TAU = 1e-3            # soft update parameter\n",
    "E_DECAY = 0.995       # Îµ decay rate for Îµ-greedy policy\n",
    "E_MIN = 0.01          # minimum Îµ value for Îµ-greedy policy\n",
    "\n",
    "num_episodes = 2000\n",
    "max_num_timesteps = 1000\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100    # number of total points to use for averaging\n",
    "epsilon = 1.0     # initial Îµ value for Îµ-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "weights = q_network.state_dict()\n",
    "target_q_network.load_state_dict(weights)\n",
    "\n",
    "for source_param, target_param in zip(q_network.parameters(), target_q_network.parameters()):\n",
    "    assert torch.all(torch.eq(source_param, target_param))\n",
    "#print(weights)\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_points = 0\n",
    "    \n",
    "    for j in range(max_num_timesteps):\n",
    "    \n",
    "        actions = q_network(torch.from_numpy(state[0]))\n",
    "\n",
    "        print(actions)\n",
    "        #print(hello)\n",
    "        #e-greedy policy\n",
    "        if random.random() > epsilon:\n",
    "            new_action=torch.argmax(actions)\n",
    "            \n",
    "        else:\n",
    "            new_action=torch.argmax(actions)\n",
    "            \n",
    "\n",
    "        epsilon = max(E_MIN, E_DECAY*epsilon)\n",
    "        new_states = env.step(new_action.item())\n",
    "\n",
    "        memory_buffer.append(experience(state,new_action,reward,new_state))\n",
    "        \n",
    "        print(new_states)\n",
    "\n",
    "        if (j + 1)%4 == 0:\n",
    "\n",
    "        print(val)        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "74c14147-8df7-4a26-bfe2-7043e8eded15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([3, 6])\n",
      "tensor([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# find the maximum value and index of the tensor along the second dimension\n",
    "max_val, max_idx = torch.max(x, dim=1)\n",
    "\n",
    "print(x)\n",
    "print(max_val)\n",
    "print(max_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c7811956-0d31-4a7d-a5b4-57156566ad6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "8\n",
      "12\n",
      "16\n",
      "20\n",
      "24\n",
      "28\n",
      "32\n",
      "36\n",
      "40\n",
      "44\n",
      "48\n",
      "52\n",
      "56\n",
      "60\n",
      "64\n",
      "68\n",
      "72\n",
      "76\n",
      "80\n",
      "84\n",
      "88\n",
      "92\n",
      "96\n",
      "100\n",
      "104\n",
      "108\n",
      "112\n",
      "116\n",
      "120\n",
      "124\n",
      "128\n",
      "132\n",
      "136\n",
      "140\n",
      "144\n",
      "148\n",
      "152\n",
      "156\n",
      "160\n",
      "164\n",
      "168\n",
      "172\n",
      "176\n",
      "180\n",
      "184\n",
      "188\n",
      "192\n",
      "196\n",
      "200\n",
      "204\n",
      "208\n",
      "212\n",
      "216\n",
      "220\n",
      "224\n",
      "228\n",
      "232\n",
      "236\n",
      "240\n",
      "244\n",
      "248\n",
      "252\n",
      "256\n",
      "260\n",
      "264\n",
      "268\n",
      "272\n",
      "276\n",
      "280\n",
      "284\n",
      "288\n",
      "292\n",
      "296\n",
      "300\n",
      "304\n",
      "308\n",
      "312\n",
      "316\n",
      "320\n",
      "324\n",
      "328\n",
      "332\n",
      "336\n",
      "340\n",
      "344\n",
      "348\n",
      "352\n",
      "356\n",
      "360\n",
      "364\n",
      "368\n",
      "372\n",
      "376\n",
      "380\n",
      "384\n",
      "388\n",
      "392\n",
      "396\n",
      "400\n",
      "404\n",
      "408\n",
      "412\n",
      "416\n",
      "420\n",
      "424\n",
      "428\n",
      "432\n",
      "436\n",
      "440\n",
      "444\n",
      "448\n",
      "452\n",
      "456\n",
      "460\n",
      "464\n",
      "468\n",
      "472\n",
      "476\n",
      "480\n",
      "484\n",
      "488\n",
      "492\n",
      "496\n",
      "500\n",
      "504\n",
      "508\n",
      "512\n",
      "516\n",
      "520\n",
      "524\n",
      "528\n",
      "532\n",
      "536\n",
      "540\n",
      "544\n",
      "548\n",
      "552\n",
      "556\n",
      "560\n",
      "564\n",
      "568\n",
      "572\n",
      "576\n",
      "580\n",
      "584\n",
      "588\n",
      "592\n",
      "596\n",
      "600\n",
      "604\n",
      "608\n",
      "612\n",
      "616\n",
      "620\n",
      "624\n",
      "628\n",
      "632\n",
      "636\n",
      "640\n",
      "644\n",
      "648\n",
      "652\n",
      "656\n",
      "660\n",
      "664\n",
      "668\n",
      "672\n",
      "676\n",
      "680\n",
      "684\n",
      "688\n",
      "692\n",
      "696\n",
      "700\n",
      "704\n",
      "708\n",
      "712\n",
      "716\n",
      "720\n",
      "724\n",
      "728\n",
      "732\n",
      "736\n",
      "740\n",
      "744\n",
      "748\n",
      "752\n",
      "756\n",
      "760\n",
      "764\n",
      "768\n",
      "772\n",
      "776\n",
      "780\n",
      "784\n",
      "788\n",
      "792\n",
      "796\n",
      "800\n",
      "804\n",
      "808\n",
      "812\n",
      "816\n",
      "820\n",
      "824\n",
      "828\n",
      "832\n",
      "836\n",
      "840\n",
      "844\n",
      "848\n",
      "852\n",
      "856\n",
      "860\n",
      "864\n",
      "868\n",
      "872\n",
      "876\n",
      "880\n",
      "884\n",
      "888\n",
      "892\n",
      "896\n",
      "900\n",
      "904\n",
      "908\n",
      "912\n",
      "916\n",
      "920\n",
      "924\n",
      "928\n",
      "932\n",
      "936\n",
      "940\n",
      "944\n",
      "948\n",
      "952\n",
      "956\n",
      "960\n",
      "964\n",
      "968\n",
      "972\n",
      "976\n",
      "980\n",
      "984\n",
      "988\n",
      "992\n",
      "996\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for i in range(max_num_timesteps):\n",
    "    counter = counter + 1\n",
    "    if (i + 1) % 4 == 0:\n",
    "        print(counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8336101-def3-48ef-8033-2a17f92d2ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
