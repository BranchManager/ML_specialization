{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4257cde4-9c87-48f9-bddc-3f1ab2bfcc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:19:23.772597: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-08 15:19:24.791773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "#import gym\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "import random\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import public_tests\n",
    "#from tensorflow.keras import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Input\n",
    "#from tensorflow.keras.losses import MSE\n",
    "#from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26e56790-4bdd-4cbc-a6ec-c67e4e06e867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb5dd300670>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Display(visible=0, size=(840, 480)).start();\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "906efc0a-9de1-4081-85ee-67cf39017726",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "723f6ade-73d4-4622-8e0a-62069c0836d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d6c7e2f-c564-4b5c-a471-8ef7bdee9dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Shape: (8,)\n",
      "Number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "\n",
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print('State Shape:', state_size)\n",
    "print('Number of actions:', num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a2af14c-d74f-4f60-917a-95272c33da94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tuple\n",
      "<class 'tuple'>\n",
      "Initial State: (array([-0.004, 1.421, -0.382, 0.436, 0.004, 0.086, 0.000, 0.000],\n",
      "      dtype=float32), {})\n",
      "Action: 0\n",
      "Next State: [-0.008 1.430 -0.381 0.410 0.009 0.085 0.000 0.000]\n",
      "Reward Received: 0.5920539161002694\n",
      "Episode Terminated: False\n",
      "Info: False\n",
      "F : {}\n"
     ]
    }
   ],
   "source": [
    "# Select an action\n",
    "action = 0\n",
    "\n",
    "initial_state = env.reset()\n",
    "\n",
    "# Run a single time step of the environment's dynamics with the given action.\n",
    "next_state, reward, done, info, f = env.step(action)\n",
    "#tuple = env.step(action)\n",
    "print(\"The tuple\")\n",
    "print(tuple)\n",
    "\n",
    "with np.printoptions(formatter={'float': '{:.3f}'.format}):\n",
    "    print(\"Initial State:\", initial_state)\n",
    "    print(\"Action:\", action)\n",
    "    print(\"Next State:\", next_state)\n",
    "    print(\"Reward Received:\", reward)\n",
    "    print(\"Episode Terminated:\", done)\n",
    "    print(\"Info:\", info)\n",
    "    print(\"F :\",f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0d6a291-048c-4569-af6a-b6c870821053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNQ_C1\n",
    "# # GRADED CELL\n",
    "\n",
    "# # Create the Q-Network\n",
    "# print(\"State size :\",type(state_size))\n",
    "# q_network = Sequential([\n",
    "#     ### START CODE HERE ### \n",
    "#     Input(shape=state_size),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=num_actions,activation='linear'),\n",
    "    \n",
    "#     ### END CODE HERE ### \n",
    "#     ])\n",
    "\n",
    "# # Create the target Q^-Network\n",
    "# target_q_network = Sequential([\n",
    "#     ### START CODE HERE ### \n",
    "#     Input(shape=state_size),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=num_actions,activation='linear'),\n",
    "#     ### END CODE HERE ###\n",
    "#     ])\n",
    "\n",
    "# ### START CODE HERE ### \n",
    "# optimizer = Adam(learning_rate=ALPHA)\n",
    "# ### END CODE HERE ###\n",
    "\n",
    "# print(q_network.summary(expand_nested=True))\n",
    "\n",
    "# print(target_q_network.summary())\n",
    "\n",
    "# this is how you'd presumably create a dense nn on torch using the same\n",
    "# layout as tf\n",
    "q_network = nn.Sequential(\n",
    "    nn.Linear(state_size[0], 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 4),\n",
    "    \n",
    ")\n",
    "\n",
    "target_q_network = nn.Sequential(\n",
    "    nn.Linear(state_size[0], 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 4),\n",
    "    \n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(q_network.parameters(), lr=ALPHA)\n",
    "optimizer = torch.optim.Adam(target_q_network.parameters(), lr=ALPHA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de3ca19b-c8c0-4894-ae21-9b3b15470367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 2. 1. 2. 0. 2. 3. 3. 2. 3. 3. 2. 0. 0. 0. 1. 1. 3. 0. 3. 3. 3. 2.\n",
      " 2. 1. 0. 1. 1. 2. 1. 1. 0. 0. 3. 1. 3. 0. 1. 2. 3. 1. 0. 1. 2. 2. 3. 0.\n",
      " 0. 2. 3. 0. 2. 2. 1. 3. 2. 0. 1. 3. 3. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "np.random.seed(1)\n",
    "states = np.float32(np.random.rand(64, 8))\n",
    "actions = np.float32(np.floor(np.random.uniform(0, 1, (64, )) * 4))\n",
    "rewards = np.float32(np.random.rand(64, ))\n",
    "next_states = np.float32(np.random.rand(64, 8))\n",
    "done_vals = np.float32((np.random.uniform(0, 1, size=(64,)) > 0.96) * 1)\n",
    "\n",
    "print(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c4b5f073-ffce-43d9-88db-6336f277ebb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.float32'>\n",
      "END\n",
      "nex_tstates type <class 'numpy.ndarray'>\n",
      "[[0.08010364 0.904631   0.37015417 0.53069746 0.49411628 0.13216114\n",
      "  0.20645405 0.07618881]\n",
      " [0.5079217  0.26154956 0.3570616  0.10806533 0.7875518  0.10658388\n",
      "  0.98570883 0.17716116]\n",
      " [0.5724051  0.04484534 0.7871163  0.18960595 0.527904   0.74007756\n",
      "  0.14993149 0.5510872 ]\n",
      " [0.21661721 0.75919604 0.7229152  0.17654903 0.86196655 0.0197751\n",
      "  0.860237   0.5589038 ]\n",
      " [0.40322047 0.7587469  0.716929   0.98732615 0.27808505 0.00379367\n",
      "  0.9339026  0.8578971 ]\n",
      " [0.7288509  0.51668876 0.70695627 0.78052956 0.37487593 0.7703225\n",
      "  0.7506243  0.6132112 ]\n",
      " [0.40186593 0.697308   0.00311286 0.7748966  0.8964166  0.2393157\n",
      "  0.12076718 0.22028399]\n",
      " [0.30209672 0.8830285  0.54316646 0.28671166 0.13835469 0.29014447\n",
      "  0.6138711  0.32413852]\n",
      " [0.45736018 0.4441171  0.8281354  0.42634815 0.34569883 0.6749716\n",
      "  0.22148205 0.46724582]\n",
      " [0.3147657  0.6268556  0.87736046 0.447689   0.78445745 0.4569657\n",
      "  0.6562293  0.13184097]\n",
      " [0.43298152 0.909312   0.605479   0.7667746  0.5047006  0.49805564\n",
      "  0.84289986 0.06780694]\n",
      " [0.5732723  0.94276255 0.51786005 0.19446582 0.8479394  0.25163913\n",
      "  0.70072603 0.5402609 ]\n",
      " [0.94883627 0.6243367  0.83797795 0.00793288 0.98934007 0.07771457\n",
      "  0.32212952 0.9461524 ]\n",
      " [0.0089391  0.82273    0.86121166 0.4398308  0.2557452  0.8026895\n",
      "  0.477862   0.1343386 ]\n",
      " [0.927849   0.89597    0.49154514 0.8567025  0.41857803 0.6834649\n",
      "  0.39799064 0.505742  ]\n",
      " [0.1895517  0.96498895 0.29421568 0.10345956 0.1443154  0.01409229\n",
      "  0.7159457  0.5644983 ]\n",
      " [0.7945784  0.5070799  0.79182106 0.69576424 0.7778485  0.40648288\n",
      "  0.64777064 0.1797943 ]\n",
      " [0.32181996 0.17260462 0.40863723 0.24141875 0.40692198 0.97522235\n",
      "  0.32031932 0.98249096]\n",
      " [0.6363061  0.37509102 0.8574845  0.61958677 0.25203308 0.7928557\n",
      "  0.43293852 0.35751116]\n",
      " [0.33027694 0.69736886 0.2686501  0.808278   0.2952888  0.5441214\n",
      "  0.4879215  0.8553564 ]\n",
      " [0.8883864  0.18438444 0.5853485  0.89820504 0.44611722 0.9218683\n",
      "  0.2789909  0.60883117]\n",
      " [0.6824537  0.22820574 0.01376751 0.41672397 0.93848187 0.3430281\n",
      "  0.77974427 0.1747363 ]\n",
      " [0.34195283 0.14459772 0.7167708  0.6993076  0.6884973  0.25339603\n",
      "  0.6923601  0.22729754]\n",
      " [0.42464912 0.37192214 0.3553079  0.05765481 0.63164663 0.7073166\n",
      "  0.6135887  0.64831275]\n",
      " [0.16994071 0.1494468  0.51417506 0.8753327  0.18395343 0.46283913\n",
      "  0.42893234 0.49728918]\n",
      " [0.16151077 0.34244063 0.2618804  0.84452695 0.80033225 0.42663917\n",
      "  0.6070155  0.14546561]\n",
      " [0.5096133  0.2969471  0.85965097 0.6715984  0.633474   0.1247513\n",
      "  0.47058788 0.9865728 ]\n",
      " [0.94829917 0.64508563 0.15172487 0.63912684 0.5656621  0.46866584\n",
      "  0.42803746 0.59926975]\n",
      " [0.84996986 0.75112104 0.57936054 0.9247042  0.06473998 0.99134654\n",
      "  0.05299454 0.19949554]\n",
      " [0.42275265 0.10750888 0.6236704  0.04799256 0.2846239  0.06103668\n",
      "  0.70351934 0.6684562 ]\n",
      " [0.3785806  0.18819426 0.7470048  0.3403793  0.79530114 0.48790094\n",
      "  0.52566946 0.02849085]\n",
      " [0.64423203 0.3506565  0.22920503 0.43388337 0.38246745 0.46978903\n",
      "  0.97948337 0.36437806]\n",
      " [0.7744101  0.5527676  0.88913107 0.35495284 0.24551868 0.91101927\n",
      "  0.04353426 0.95075345]\n",
      " [0.5564069  0.37636322 0.99505234 0.05836265 0.51670635 0.03109707\n",
      "  0.57117575 0.18046851]\n",
      " [0.6309592  0.98092365 0.87490255 0.45183626 0.70846087 0.77746874\n",
      "  0.4948431  0.5285335 ]\n",
      " [0.1507844  0.36939994 0.14222126 0.7268938  0.477013   0.44887882\n",
      "  0.88599795 0.52761877]\n",
      " [0.40909085 0.26889202 0.07201204 0.41813612 0.02575348 0.29115394\n",
      "  0.5035095  0.9659331 ]\n",
      " [0.10938291 0.67304105 0.49993238 0.77709824 0.14360699 0.08320264\n",
      "  0.39921862 0.79696226]\n",
      " [0.19167574 0.7677772  0.29029799 0.21689148 0.01671559 0.39865905\n",
      "  0.3810815  0.6593449 ]\n",
      " [0.0709184  0.15260398 0.01657588 0.11379635 0.65178925 0.40265685\n",
      "  0.32102633 0.557912  ]\n",
      " [0.9934605  0.8344865  0.69962317 0.9182586  0.0397287  0.07033344\n",
      "  0.4740063  0.3491674 ]\n",
      " [0.93725204 0.48956496 0.5396491  0.8952604  0.44663504 0.87703437\n",
      "  0.25358176 0.2738097 ]\n",
      " [0.3283614  0.54756427 0.22012867 0.67142916 0.14279328 0.09410027\n",
      "  0.87019175 0.23686871]\n",
      " [0.386004   0.5715421  0.52580196 0.07602388 0.87412596 0.95113564\n",
      "  0.81250733 0.28380182]\n",
      " [0.5278468  0.3394167  0.5546673  0.97440344 0.3117029  0.6687966\n",
      "  0.3259672  0.77447724]\n",
      " [0.32580996 0.8898274  0.75170773 0.7626321  0.46947902 0.2107645\n",
      "  0.04147508 0.3218288 ]\n",
      " [0.03711266 0.6938554  0.67035    0.43047178 0.767789   0.5360085\n",
      "  0.03985993 0.13479312]\n",
      " [0.1934164  0.3356638  0.05231295 0.6051168  0.512061   0.617461\n",
      "  0.43235558 0.8477005 ]\n",
      " [0.45405906 0.01540352 0.87306815 0.65620154 0.82300305 0.95177567\n",
      "  0.05091238 0.23507187]\n",
      " [0.06334344 0.4216579  0.86382914 0.08162399 0.47311196 0.1255431\n",
      "  0.7728856  0.84142214]\n",
      " [0.04329094 0.48644075 0.23941104 0.95247376 0.9438926  0.613934\n",
      "  0.9734874  0.34486136]\n",
      " [0.8978507  0.43459496 0.23581463 0.94082797 0.68421805 0.06491159\n",
      "  0.87042487 0.7013803 ]\n",
      " [0.6049269  0.7323749  0.25343904 0.60048896 0.8146192  0.05411366\n",
      "  0.13051067 0.8424458 ]\n",
      " [0.61834586 0.5312878  0.24829073 0.29507858 0.87268615 0.42166594\n",
      "  0.06443026 0.89698505]\n",
      " [0.20338084 0.82622755 0.8817706  0.48675075 0.59846467 0.52726746\n",
      "  0.6248213  0.85504174]\n",
      " [0.28213945 0.8837555  0.5676903  0.11510304 0.22700116 0.5959824\n",
      "  0.23944624 0.13141567]\n",
      " [0.16184771 0.84487253 0.60218364 0.9635671  0.34567922 0.5956251\n",
      "  0.5989853  0.6157043 ]\n",
      " [0.05917721 0.75031716 0.9482095  0.53467906 0.19255602 0.75292593\n",
      "  0.00731897 0.3282561 ]\n",
      " [0.91760635 0.58836734 0.8551903  0.6046721  0.82257813 0.8794835\n",
      "  0.32098636 0.12295477]\n",
      " [0.72130334 0.44034708 0.1267357  0.58982366 0.03606831 0.20018211\n",
      "  0.78830117 0.01209676]\n",
      " [0.30334568 0.02137596 0.997485   0.58203    0.29337627 0.9289495\n",
      "  0.5071194  0.4546922 ]\n",
      " [0.5878715  0.26413766 0.30528829 0.37165457 0.244448   0.5845473\n",
      "  0.69584614 0.07194722]\n",
      " [0.97108334 0.75307065 0.8061632  0.7516404  0.08006097 0.48169628\n",
      "  0.44567296 0.67247266]\n",
      " [0.44873726 0.7043134  0.68164533 0.6971485  0.6186007  0.15098073\n",
      "  0.76080245 0.7810458 ]]\n",
      "type of n_statae\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "#     \"\"\" \n",
    "#     Calculates the loss.\n",
    "    \n",
    "#     Args:\n",
    "#       experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "#       gamma: (float) The discount factor.\n",
    "#       q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "#       target_q_network: (tf.keras.Sequential) Karas model for predicting the targets\n",
    "          \n",
    "#     Returns:\n",
    "#       loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "#             the y targets and the Q(s,a) values.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Unpack the mini-batch of experience tuples\n",
    "#     states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "#     # Compute max Q^(s,a)\n",
    "#     max_qsa = tf.reduce_max(target_q_network(next_states), axis=0)\n",
    "\n",
    "#     print(type(target_q_network(next_states)))\n",
    "    \n",
    "#     print(target_q_network(next_states))\n",
    "#     print(\"numpy shape \",target_q_network(next_states).shape)\n",
    "#     print(\"numpy max, \" ,np.max(target_q_network(next_states),axis=0))\n",
    "#     print(\"numpy dtype , \",target_q_network(next_states).dtype)\n",
    "#     print(max_qsa)\n",
    "    \n",
    "#     # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a).\n",
    "#     ### START CODE HERE ### \n",
    "#     y = (rewards + (1-done_vals)*(gamma*max_qsa))\n",
    "#     ### END CODE HERE ###\n",
    "    \n",
    "#     # Get the q_values\n",
    "#     q_values = q_network(states)\n",
    "#     q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "#                                                 tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "#     # Compute the loss\n",
    "#     ### START CODE HERE ### \n",
    "#     loss = MSE(y, q_values)\n",
    "#     ### END CODE HERE ### \n",
    "    \n",
    "#     return loss\n",
    "\n",
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \n",
    "    # Unpack the mini-batch of experience tuples\n",
    "    \n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "   \n",
    "    #input_tensor = torch.randn(20,20)\n",
    "    #print(type(input_tensor))\n",
    "    #print(input_tensor)\n",
    "    print(type(states))\n",
    "    print(type(next_states))\n",
    "    print(type(next_states[0][0]))\n",
    "   \n",
    "    print(\"END\")\n",
    "    print(\"nex_tstates type\", type(next_states))\n",
    "    print(next_states)\n",
    "    n_statae = target_q_network(torch.from_numpy(next_states))\n",
    "\n",
    "    print(\"type of n_statae\")\n",
    "    print(type(n_statae))\n",
    "\n",
    "    #print(\"max of \", n_statae)\n",
    "    max_qsa , indicies  = torch.max(torch.from_numpy(n_statae), dim=1)\n",
    "    \n",
    "    #print(\"Actions :\",actions)\n",
    "\n",
    "    #print(\"max_qsa \", max_qsa)\n",
    "    y = (rewards + (1-done_vals)*(gamma*max_qsa.numpy()))\n",
    "    \n",
    "    q_values = q_network(states)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #print(\"q_values ,\",q_values)\n",
    "    #print(\"action shape \",type(actions))\n",
    "    #print(np.arange(actions.shape[0]))\n",
    "    #print(\"q_values 64, \",q_values[np.arange(actions.shape[0]),actions.astype(int)])\n",
    "    q_values = q_values[np.arange(actions.shape[0]),actions.astype(int)]\n",
    "    #print(\"Y ,\",y.shape)\n",
    "    #print(\"q_values, \",q_values.size())\n",
    "    mseloss = torch.nn.MSELoss()\n",
    "    loss = mseloss(torch.from_numpy(y),torch.from_numpy(q_values))\n",
    "    #mse_loss = torch.nn.functional.mse_loss(y,q_values)\n",
    "\n",
    "\n",
    "\n",
    "    #agent learn section\n",
    "\n",
    "    \n",
    "\n",
    "    #print(loss)\n",
    "    return loss\n",
    "\n",
    "public_tests.test_compute_loss(compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b83f62bd-1b5d-4299-b493-0e7c88615e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "25bda1ea-aa62-45ee-ab55-d7ae1d30b834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.float32'>\n",
      "[[ 2.72480398e-01  9.66688871e-01  4.38130677e-01 -1.21633744e+00\n",
      "   2.72438216e+00  2.23320246e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 9.31293517e-02  1.47354925e+00  5.91604531e-01  2.43150014e-02\n",
      "   8.21204185e-02  3.88233244e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 5.52471168e-02  1.46110427e+00  6.52403057e-01  1.86133087e-01\n",
      "  -4.07340284e-03  1.45369977e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.62351900e-01  1.04404080e+00  4.15176779e-01 -1.12826693e+00\n",
      "   2.39513779e+00  2.12732172e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.18591601e-01  1.28463018e+00  3.81186903e-01 -7.30546057e-01\n",
      "   1.28293765e+00  1.51526141e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.26483822e-01  1.25181985e+00  3.76607329e-01 -7.99359262e-01\n",
      "   1.44297993e+00  1.63116431e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.45065019e-01  1.15726650e+00  3.84297431e-01 -9.67092097e-01\n",
      "   1.88898897e+00  1.88743055e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.29190165e-01  5.13842463e-01  5.42313516e-01 -1.54711008e+00\n",
      "   4.49332619e+00  2.78422117e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.74476624e-01  1.40800238e+00  4.36969697e-01 -4.11221713e-01\n",
      "   6.35808766e-01  1.06362700e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.43310549e-02  1.42703581e+00  7.17967272e-01  3.45187962e-01\n",
      "  -1.42341089e-02 -1.17714539e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.82868952e-01  8.82062852e-01  4.69189107e-01 -1.30007958e+00\n",
      "   3.07228446e+00  2.35552859e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.48597816e-01  1.13614166e+00  3.87958139e-01 -1.00000238e+00\n",
      "   1.98577273e+00  1.93568635e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.10206324e-01  6.57484531e-01  5.29067159e-01 -1.46477723e+00\n",
      "   3.94854951e+00  2.61867309e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.05102539e-01  1.47284389e+00  5.70080161e-01 -3.12504806e-02\n",
      "   1.27242222e-01  4.75077778e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.64758116e-01  1.42493904e+00  4.55707014e-01 -3.49886358e-01\n",
      "   5.32069862e-01  9.68581498e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.90091604e-01  8.21665347e-01  4.86557186e-01 -1.35110772e+00\n",
      "   3.31276631e+00  2.41966391e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.93038374e-01  1.36655200e+00  4.11528498e-01 -5.34189224e-01\n",
      "   8.66883039e-01  1.22032499e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.69102752e-01  9.93274808e-01  4.28995192e-01 -1.18680513e+00\n",
      "   2.61272264e+00  2.19219542e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.22575665e-01  1.26858473e+00  3.78535032e-01 -7.64448166e-01\n",
      "   1.36142194e+00  1.56969035e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.97729880e-01  7.58187711e-01  5.05816460e-01 -1.39850891e+00\n",
      "   3.56104922e+00  2.49846482e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.75896847e-01  9.39285159e-01  4.49463606e-01 -1.24552035e+00\n",
      "   2.83841586e+00  2.28069520e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.55527198e-01  1.09167540e+00  3.98477495e-01 -1.06280780e+00\n",
      "   2.18514776e+00  2.01614571e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.51968780e-02  1.44675076e+00  6.86185658e-01  2.65736401e-01\n",
      "  -1.94109753e-02  1.00200074e-02  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.33494943e-01  1.46043086e+00  5.16273260e-01 -1.71133563e-01\n",
      "   2.78520882e-01  6.94385231e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 8.70174393e-02  1.47298121e+00  6.01489604e-01  5.18798679e-02\n",
      "   6.27105236e-02  3.48543316e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 8.08265656e-02  1.47179973e+00  6.13452792e-01  7.84699619e-02\n",
      "   4.52849679e-02  3.00888121e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.38927937e-01  1.45609975e+00  5.07990479e-01 -1.99646518e-01\n",
      "   3.14992756e-01  7.29503334e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 6.17548004e-02  1.46468472e+00  6.41144156e-01  1.59131259e-01\n",
      "   5.44540212e-03  1.90393597e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.97497457e-01  1.35455763e+00  4.05769438e-01 -5.66478908e-01\n",
      "   9.30233300e-01  1.26700795e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.79224104e-01  1.39858723e+00  4.30818856e-01 -4.41191286e-01\n",
      "   6.90700531e-01  1.09783673e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.37815946e-01  1.19729614e+00  3.78798902e-01 -9.00695682e-01\n",
      "   1.70234799e+00  1.78879035e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.14531422e-01  1.29997826e+00  3.84856135e-01 -6.96436644e-01\n",
      "   1.20717478e+00  1.45842302e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.59771726e-01  1.43245506e+00  4.65816587e-01 -3.18814635e-01\n",
      "   4.83640879e-01  9.09162402e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.79359043e-01  9.11074877e-01  4.60113585e-01 -1.27327657e+00\n",
      "   2.95450926e+00  2.32188845e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 6.81914315e-02  1.46765423e+00  6.32242560e-01  1.31898716e-01\n",
      "   1.67418607e-02  2.25949734e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.16720483e-01  1.46971118e+00  5.49839795e-01 -8.60560164e-02\n",
      "   1.81221232e-01  5.56136549e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.13776585e-02  1.43419981e+00  7.08733618e-01  3.18355024e-01\n",
      "  -1.82618666e-02 -8.05629790e-02  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.88507363e-01  1.37788522e+00  4.18509960e-01 -5.01515925e-01\n",
      "   8.05866897e-01  1.16878414e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.19346130e-01  5.86903989e-01  5.40218711e-01 -1.50587499e+00\n",
      "   4.21700096e+00  2.70815945e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.06171036e-01  1.32859945e+00  3.93923044e-01 -6.30926490e-01\n",
      "   1.06397021e+00  1.36164677e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.52083689e-01  1.11428988e+00  3.92832100e-01 -1.03085732e+00\n",
      "   2.08434105e+00  1.97138238e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.10392565e-01  1.31462920e+00  3.89573216e-01 -6.63103342e-01\n",
      "   1.13425386e+00  1.40567589e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.14684004e-01  6.22516692e-01  5.35043836e-01 -1.48566127e+00\n",
      "   4.08163691e+00  2.66178346e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.10944651e-01  1.47157776e+00  5.57984591e-01 -5.87172285e-02\n",
      "   1.53416932e-01  5.23541749e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.41473392e-01  1.17764270e+00  3.81811827e-01 -9.34864819e-01\n",
      "   1.79461789e+00  1.84540617e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.34394634e-01  4.76475984e-01  5.42838216e-01 -1.56637013e+00\n",
      "   4.63490057e+00  2.83408117e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.30325893e-01  1.23437989e+00  3.76576751e-01 -8.31244111e-01\n",
      "   1.52643955e+00  1.66920018e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 7.45399445e-02  1.47002566e+00  6.21206164e-01  1.05192065e-01\n",
      "   3.02419178e-02  2.70025909e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.27992719e-01  1.46414638e+00  5.27344763e-01 -1.42179683e-01\n",
      "   2.43804768e-01  6.47718191e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.93859780e-01  7.90305555e-01  4.98564422e-01 -1.37508202e+00\n",
      "   3.43612742e+00  2.46724939e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.69664294e-01  1.41679418e+00  4.47079837e-01 -3.79976481e-01\n",
      "   5.82627475e-01  1.01115394e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 4.19723503e-02  1.45213652e+00  6.74715102e-01  2.39401445e-01\n",
      "  -1.66086853e-02  5.60511835e-02  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.01874644e-01  1.34191036e+00  3.99568498e-01 -5.98420918e-01\n",
      "   9.95887935e-01  1.31309676e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.86444485e-01  8.52256536e-01  4.78565753e-01 -1.32600594e+00\n",
      "   3.19178462e+00  2.39002657e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 4.86495979e-02  1.45691717e+00  6.62398815e-01  2.12515041e-01\n",
      "  -1.13412319e-02  1.05358616e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.05899531e-01  6.91775024e-01  5.23642957e-01 -1.44239128e+00\n",
      "   3.81761765e+00  2.58625364e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.49532601e-01  1.44555175e+00  4.86944824e-01 -2.58639187e-01\n",
      "   3.94740999e-01  8.21876884e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.34102339e-01  1.21620750e+00  3.77148122e-01 -8.66046011e-01\n",
      "   1.61290884e+00  1.72939241e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.24178874e-01  5.50671518e-01  5.41764975e-01 -1.52635229e+00\n",
      "   4.35423803e+00  2.74692440e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.83898836e-01  1.38854897e+00  4.24130589e-01 -4.71295446e-01\n",
      "   7.47427821e-01  1.13454759e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.01740468e-01  7.25345135e-01  5.15573144e-01 -1.42028010e+00\n",
      "   3.68830681e+00  2.54518700e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.65735626e-01  1.01905227e+00  4.22147840e-01 -1.15747046e+00\n",
      "   2.50311375e+00  2.15953445e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.54700845e-01  1.43932319e+00  4.76595074e-01 -2.88613170e-01\n",
      "   4.38184947e-01  8.68957818e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.22400090e-01  1.46723545e+00  5.37998557e-01 -1.13977589e-01\n",
      "   2.11421788e-01  6.04066133e-01  0.00000000e+00  0.00000000e+00]]\n",
      "yoooooo we here aya\n",
      "yoooo this aint it\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.float32'>\n",
      "END\n",
      "nex_tstates type <class 'numpy.ndarray'>\n",
      "[[ 2.72480398e-01  9.66688871e-01  4.38130677e-01 -1.21633744e+00\n",
      "   2.72438216e+00  2.23320246e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 9.31293517e-02  1.47354925e+00  5.91604531e-01  2.43150014e-02\n",
      "   8.21204185e-02  3.88233244e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 5.52471168e-02  1.46110427e+00  6.52403057e-01  1.86133087e-01\n",
      "  -4.07340284e-03  1.45369977e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.62351900e-01  1.04404080e+00  4.15176779e-01 -1.12826693e+00\n",
      "   2.39513779e+00  2.12732172e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.18591601e-01  1.28463018e+00  3.81186903e-01 -7.30546057e-01\n",
      "   1.28293765e+00  1.51526141e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.26483822e-01  1.25181985e+00  3.76607329e-01 -7.99359262e-01\n",
      "   1.44297993e+00  1.63116431e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.45065019e-01  1.15726650e+00  3.84297431e-01 -9.67092097e-01\n",
      "   1.88898897e+00  1.88743055e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.29190165e-01  5.13842463e-01  5.42313516e-01 -1.54711008e+00\n",
      "   4.49332619e+00  2.78422117e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.74476624e-01  1.40800238e+00  4.36969697e-01 -4.11221713e-01\n",
      "   6.35808766e-01  1.06362700e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.43310549e-02  1.42703581e+00  7.17967272e-01  3.45187962e-01\n",
      "  -1.42341089e-02 -1.17714539e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.82868952e-01  8.82062852e-01  4.69189107e-01 -1.30007958e+00\n",
      "   3.07228446e+00  2.35552859e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.48597816e-01  1.13614166e+00  3.87958139e-01 -1.00000238e+00\n",
      "   1.98577273e+00  1.93568635e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.10206324e-01  6.57484531e-01  5.29067159e-01 -1.46477723e+00\n",
      "   3.94854951e+00  2.61867309e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.05102539e-01  1.47284389e+00  5.70080161e-01 -3.12504806e-02\n",
      "   1.27242222e-01  4.75077778e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.64758116e-01  1.42493904e+00  4.55707014e-01 -3.49886358e-01\n",
      "   5.32069862e-01  9.68581498e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.90091604e-01  8.21665347e-01  4.86557186e-01 -1.35110772e+00\n",
      "   3.31276631e+00  2.41966391e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.93038374e-01  1.36655200e+00  4.11528498e-01 -5.34189224e-01\n",
      "   8.66883039e-01  1.22032499e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.69102752e-01  9.93274808e-01  4.28995192e-01 -1.18680513e+00\n",
      "   2.61272264e+00  2.19219542e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.22575665e-01  1.26858473e+00  3.78535032e-01 -7.64448166e-01\n",
      "   1.36142194e+00  1.56969035e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.97729880e-01  7.58187711e-01  5.05816460e-01 -1.39850891e+00\n",
      "   3.56104922e+00  2.49846482e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.75896847e-01  9.39285159e-01  4.49463606e-01 -1.24552035e+00\n",
      "   2.83841586e+00  2.28069520e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.55527198e-01  1.09167540e+00  3.98477495e-01 -1.06280780e+00\n",
      "   2.18514776e+00  2.01614571e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.51968780e-02  1.44675076e+00  6.86185658e-01  2.65736401e-01\n",
      "  -1.94109753e-02  1.00200074e-02  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.33494943e-01  1.46043086e+00  5.16273260e-01 -1.71133563e-01\n",
      "   2.78520882e-01  6.94385231e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 8.70174393e-02  1.47298121e+00  6.01489604e-01  5.18798679e-02\n",
      "   6.27105236e-02  3.48543316e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 8.08265656e-02  1.47179973e+00  6.13452792e-01  7.84699619e-02\n",
      "   4.52849679e-02  3.00888121e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.38927937e-01  1.45609975e+00  5.07990479e-01 -1.99646518e-01\n",
      "   3.14992756e-01  7.29503334e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 6.17548004e-02  1.46468472e+00  6.41144156e-01  1.59131259e-01\n",
      "   5.44540212e-03  1.90393597e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.97497457e-01  1.35455763e+00  4.05769438e-01 -5.66478908e-01\n",
      "   9.30233300e-01  1.26700795e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.79224104e-01  1.39858723e+00  4.30818856e-01 -4.41191286e-01\n",
      "   6.90700531e-01  1.09783673e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.37815946e-01  1.19729614e+00  3.78798902e-01 -9.00695682e-01\n",
      "   1.70234799e+00  1.78879035e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.14531422e-01  1.29997826e+00  3.84856135e-01 -6.96436644e-01\n",
      "   1.20717478e+00  1.45842302e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.59771726e-01  1.43245506e+00  4.65816587e-01 -3.18814635e-01\n",
      "   4.83640879e-01  9.09162402e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.79359043e-01  9.11074877e-01  4.60113585e-01 -1.27327657e+00\n",
      "   2.95450926e+00  2.32188845e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 6.81914315e-02  1.46765423e+00  6.32242560e-01  1.31898716e-01\n",
      "   1.67418607e-02  2.25949734e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.16720483e-01  1.46971118e+00  5.49839795e-01 -8.60560164e-02\n",
      "   1.81221232e-01  5.56136549e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.13776585e-02  1.43419981e+00  7.08733618e-01  3.18355024e-01\n",
      "  -1.82618666e-02 -8.05629790e-02  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.88507363e-01  1.37788522e+00  4.18509960e-01 -5.01515925e-01\n",
      "   8.05866897e-01  1.16878414e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.19346130e-01  5.86903989e-01  5.40218711e-01 -1.50587499e+00\n",
      "   4.21700096e+00  2.70815945e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.06171036e-01  1.32859945e+00  3.93923044e-01 -6.30926490e-01\n",
      "   1.06397021e+00  1.36164677e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.52083689e-01  1.11428988e+00  3.92832100e-01 -1.03085732e+00\n",
      "   2.08434105e+00  1.97138238e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.10392565e-01  1.31462920e+00  3.89573216e-01 -6.63103342e-01\n",
      "   1.13425386e+00  1.40567589e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.14684004e-01  6.22516692e-01  5.35043836e-01 -1.48566127e+00\n",
      "   4.08163691e+00  2.66178346e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.10944651e-01  1.47157776e+00  5.57984591e-01 -5.87172285e-02\n",
      "   1.53416932e-01  5.23541749e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.41473392e-01  1.17764270e+00  3.81811827e-01 -9.34864819e-01\n",
      "   1.79461789e+00  1.84540617e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.34394634e-01  4.76475984e-01  5.42838216e-01 -1.56637013e+00\n",
      "   4.63490057e+00  2.83408117e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.30325893e-01  1.23437989e+00  3.76576751e-01 -8.31244111e-01\n",
      "   1.52643955e+00  1.66920018e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 7.45399445e-02  1.47002566e+00  6.21206164e-01  1.05192065e-01\n",
      "   3.02419178e-02  2.70025909e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.27992719e-01  1.46414638e+00  5.27344763e-01 -1.42179683e-01\n",
      "   2.43804768e-01  6.47718191e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.93859780e-01  7.90305555e-01  4.98564422e-01 -1.37508202e+00\n",
      "   3.43612742e+00  2.46724939e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.69664294e-01  1.41679418e+00  4.47079837e-01 -3.79976481e-01\n",
      "   5.82627475e-01  1.01115394e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 4.19723503e-02  1.45213652e+00  6.74715102e-01  2.39401445e-01\n",
      "  -1.66086853e-02  5.60511835e-02  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.01874644e-01  1.34191036e+00  3.99568498e-01 -5.98420918e-01\n",
      "   9.95887935e-01  1.31309676e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.86444485e-01  8.52256536e-01  4.78565753e-01 -1.32600594e+00\n",
      "   3.19178462e+00  2.39002657e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 4.86495979e-02  1.45691717e+00  6.62398815e-01  2.12515041e-01\n",
      "  -1.13412319e-02  1.05358616e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.05899531e-01  6.91775024e-01  5.23642957e-01 -1.44239128e+00\n",
      "   3.81761765e+00  2.58625364e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.49532601e-01  1.44555175e+00  4.86944824e-01 -2.58639187e-01\n",
      "   3.94740999e-01  8.21876884e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.34102339e-01  1.21620750e+00  3.77148122e-01 -8.66046011e-01\n",
      "   1.61290884e+00  1.72939241e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.24178874e-01  5.50671518e-01  5.41764975e-01 -1.52635229e+00\n",
      "   4.35423803e+00  2.74692440e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.83898836e-01  1.38854897e+00  4.24130589e-01 -4.71295446e-01\n",
      "   7.47427821e-01  1.13454759e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.01740468e-01  7.25345135e-01  5.15573144e-01 -1.42028010e+00\n",
      "   3.68830681e+00  2.54518700e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.65735626e-01  1.01905227e+00  4.22147840e-01 -1.15747046e+00\n",
      "   2.50311375e+00  2.15953445e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.54700845e-01  1.43932319e+00  4.76595074e-01 -2.88613170e-01\n",
      "   4.38184947e-01  8.68957818e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.22400090e-01  1.46723545e+00  5.37998557e-01 -1.13977589e-01\n",
      "   2.11421788e-01  6.04066133e-01  0.00000000e+00  0.00000000e+00]]\n",
      "type of n_statae\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got Tensor)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 85\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myoooo this aint it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;66;03m#q_network(n_states)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m         \n\u001b[1;32m     83\u001b[0m         \u001b[38;5;66;03m#agent learn part\u001b[39;00m\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;66;03m#print(list(q_network.parameters()))\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGAMMA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_q_network\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[92], line 69\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(experiences, gamma, q_network, target_q_network)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(n_statae))\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#print(\"max of \", n_statae)\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m max_qsa , indicies  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_statae\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#print(\"Actions :\",actions)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#print(\"max_qsa \", max_qsa)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m y \u001b[38;5;241m=\u001b[39m (rewards \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mdone_vals)\u001b[38;5;241m*\u001b[39m(gamma\u001b[38;5;241m*\u001b[39mmax_qsa\u001b[38;5;241m.\u001b[39mnumpy()))\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got Tensor)"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "SEED = 0              # seed for pseudo-random number generator\n",
    "MINIBATCH_SIZE = 64   # mini-batch size\n",
    "TAU = 1e-3            # soft update parameter\n",
    "E_DECAY = 0.995       # ε decay rate for ε-greedy policy\n",
    "E_MIN = 0.01          # minimum ε value for ε-greedy policy\n",
    "\n",
    "num_episodes = 2000\n",
    "max_num_timesteps = 1000\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100    # number of total points to use for averaging\n",
    "epsilon = 1.0     # initial ε value for ε-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "weights = q_network.state_dict()\n",
    "target_q_network.load_state_dict(weights)\n",
    "\n",
    "for source_param, target_param in zip(q_network.parameters(), target_q_network.parameters()):\n",
    "    assert torch.all(torch.eq(source_param, target_param))\n",
    "#print(weights)\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_points = 0\n",
    "    \n",
    "    for j in range(max_num_timesteps):\n",
    "    \n",
    "        actions = q_network(torch.from_numpy(state[0]))\n",
    "\n",
    "        #print(state[0])\n",
    "        #print(\"Theser are the actions: \",actions)\n",
    "        #print(actions)\n",
    "        #print(hello)\n",
    "        #e-greedy policy\n",
    "        if random.random() > epsilon:\n",
    "            new_action=torch.argmax(actions)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            new_action=torch.argmax(actions)\n",
    "            \n",
    "\n",
    "        epsilon = max(E_MIN, E_DECAY*epsilon)\n",
    "        new_states,reward,done,_,_ = env.step(new_action.item())\n",
    "\n",
    "        \n",
    "        memory_buffer.append(experience(state,new_action,reward,new_states,done))\n",
    "\n",
    "       \n",
    "      \n",
    "       \n",
    "\n",
    "        if (j + 1)%4 == 0 and len(memory_buffer) > 64 :\n",
    "            experiences = random.sample(memory_buffer, k=64)\n",
    "\n",
    "            #states = tf.convert_to_tensor(np.array([e.state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "            states = [e.state[0] for e in experiences]\n",
    "            states = np.stack(states) #convert list of numpy arrays to numpy array of numpy arrays\n",
    "            #print(states)\n",
    "            new_actions = [e.action for e in experiences]\n",
    "            new_actions = np.array([a.item() for a in new_actions])\n",
    "            reward = np.array([e.reward for e in experiences])\n",
    "            # n_states = np.stack([e.next_state for e in experiences]).astype(np.float32)\n",
    "            n_states = torch.tensor([e.next_state for e in experiences]).numpy()\n",
    "            done_vals = np.array([e.done for e in experiences]).astype(np.uint8)\n",
    "            print(type([e.done for e in experiences]))\n",
    "            experiences = (states,new_actions,reward,n_states,done_vals)\n",
    "            print(type(n_states[0][0]))\n",
    "            print(n_states)\n",
    "            #TODO: 11:50 am 11/8 find out what the done variable is in the experience tuple\n",
    "            print(\"yoooooo we here aya\")\n",
    "            #print(np.array([e.state for e in experiences if e is not None]))\n",
    "            #print(experiences)\n",
    "            print(\"yoooo this aint it\")\n",
    "\n",
    "            #q_network(n_states)\n",
    "            \n",
    "            #agent learn part\n",
    "            #print(list(q_network.parameters()))\n",
    "            loss = compute_loss(experiences, GAMMA, q_network, target_q_network)\n",
    "\n",
    "            break\n",
    "    break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "74c14147-8df7-4a26-bfe2-7043e8eded15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([3, 6])\n",
      "tensor([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "# print(type(x))\n",
    "# # find the maximum value and index of the tensor along the second dimension\n",
    "# max_val, max_idx = torch.max(x, dim=1)\n",
    "\n",
    "# print(x)\n",
    "# print(max_val)\n",
    "# print(max_idx)\n",
    "\n",
    "def target_q_network_random(inputs):\n",
    "    return np.float32(np.random.rand(inputs.shape[0],num_actions))\n",
    "\n",
    "def q_network_random(inputs):\n",
    "    return np.float32(np.random.rand(inputs.shape[0],num_actions))\n",
    "\n",
    "np.random.seed(1)\n",
    "states = np.float32(np.random.rand(64, 8))\n",
    "actions = np.float32(np.floor(np.random.uniform(0, 1, (64, )) * 4))\n",
    "rewards = np.float32(np.random.rand(64, ))\n",
    "next_states = np.float32(np.random.rand(64, 8))\n",
    "done_vals = np.float32((np.random.uniform(0, 1, size=(64,)) > 0.96) * 1)\n",
    "\n",
    "loss = target((states, actions, rewards, next_states, done_vals), 0.995, q_network_random, target_q_network_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c7811956-0d31-4a7d-a5b4-57156566ad6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "8\n",
      "12\n",
      "16\n",
      "20\n",
      "24\n",
      "28\n",
      "32\n",
      "36\n",
      "40\n",
      "44\n",
      "48\n",
      "52\n",
      "56\n",
      "60\n",
      "64\n",
      "68\n",
      "72\n",
      "76\n",
      "80\n",
      "84\n",
      "88\n",
      "92\n",
      "96\n",
      "100\n",
      "104\n",
      "108\n",
      "112\n",
      "116\n",
      "120\n",
      "124\n",
      "128\n",
      "132\n",
      "136\n",
      "140\n",
      "144\n",
      "148\n",
      "152\n",
      "156\n",
      "160\n",
      "164\n",
      "168\n",
      "172\n",
      "176\n",
      "180\n",
      "184\n",
      "188\n",
      "192\n",
      "196\n",
      "200\n",
      "204\n",
      "208\n",
      "212\n",
      "216\n",
      "220\n",
      "224\n",
      "228\n",
      "232\n",
      "236\n",
      "240\n",
      "244\n",
      "248\n",
      "252\n",
      "256\n",
      "260\n",
      "264\n",
      "268\n",
      "272\n",
      "276\n",
      "280\n",
      "284\n",
      "288\n",
      "292\n",
      "296\n",
      "300\n",
      "304\n",
      "308\n",
      "312\n",
      "316\n",
      "320\n",
      "324\n",
      "328\n",
      "332\n",
      "336\n",
      "340\n",
      "344\n",
      "348\n",
      "352\n",
      "356\n",
      "360\n",
      "364\n",
      "368\n",
      "372\n",
      "376\n",
      "380\n",
      "384\n",
      "388\n",
      "392\n",
      "396\n",
      "400\n",
      "404\n",
      "408\n",
      "412\n",
      "416\n",
      "420\n",
      "424\n",
      "428\n",
      "432\n",
      "436\n",
      "440\n",
      "444\n",
      "448\n",
      "452\n",
      "456\n",
      "460\n",
      "464\n",
      "468\n",
      "472\n",
      "476\n",
      "480\n",
      "484\n",
      "488\n",
      "492\n",
      "496\n",
      "500\n",
      "504\n",
      "508\n",
      "512\n",
      "516\n",
      "520\n",
      "524\n",
      "528\n",
      "532\n",
      "536\n",
      "540\n",
      "544\n",
      "548\n",
      "552\n",
      "556\n",
      "560\n",
      "564\n",
      "568\n",
      "572\n",
      "576\n",
      "580\n",
      "584\n",
      "588\n",
      "592\n",
      "596\n",
      "600\n",
      "604\n",
      "608\n",
      "612\n",
      "616\n",
      "620\n",
      "624\n",
      "628\n",
      "632\n",
      "636\n",
      "640\n",
      "644\n",
      "648\n",
      "652\n",
      "656\n",
      "660\n",
      "664\n",
      "668\n",
      "672\n",
      "676\n",
      "680\n",
      "684\n",
      "688\n",
      "692\n",
      "696\n",
      "700\n",
      "704\n",
      "708\n",
      "712\n",
      "716\n",
      "720\n",
      "724\n",
      "728\n",
      "732\n",
      "736\n",
      "740\n",
      "744\n",
      "748\n",
      "752\n",
      "756\n",
      "760\n",
      "764\n",
      "768\n",
      "772\n",
      "776\n",
      "780\n",
      "784\n",
      "788\n",
      "792\n",
      "796\n",
      "800\n",
      "804\n",
      "808\n",
      "812\n",
      "816\n",
      "820\n",
      "824\n",
      "828\n",
      "832\n",
      "836\n",
      "840\n",
      "844\n",
      "848\n",
      "852\n",
      "856\n",
      "860\n",
      "864\n",
      "868\n",
      "872\n",
      "876\n",
      "880\n",
      "884\n",
      "888\n",
      "892\n",
      "896\n",
      "900\n",
      "904\n",
      "908\n",
      "912\n",
      "916\n",
      "920\n",
      "924\n",
      "928\n",
      "932\n",
      "936\n",
      "940\n",
      "944\n",
      "948\n",
      "952\n",
      "956\n",
      "960\n",
      "964\n",
      "968\n",
      "972\n",
      "976\n",
      "980\n",
      "984\n",
      "988\n",
      "992\n",
      "996\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for i in range(max_num_timesteps):\n",
    "    counter = counter + 1\n",
    "    if (i + 1) % 4 == 0:\n",
    "        print(counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8336101-def3-48ef-8033-2a17f92d2ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
