{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4257cde4-9c87-48f9-bddc-3f1ab2bfcc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "#import gym\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import public_tests\n",
    "#from tensorflow.keras import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Input\n",
    "#from tensorflow.keras.losses import MSE\n",
    "#from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26e56790-4bdd-4cbc-a6ec-c67e4e06e867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f166c730990>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Display(visible=0, size=(840, 480)).start();\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "906efc0a-9de1-4081-85ee-67cf39017726",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "723f6ade-73d4-4622-8e0a-62069c0836d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d6c7e2f-c564-4b5c-a471-8ef7bdee9dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Shape: (8,)\n",
      "Number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "\n",
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print('State Shape:', state_size)\n",
    "print('Number of actions:', num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a2af14c-d74f-4f60-917a-95272c33da94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tuple\n",
      "<class 'tuple'>\n",
      "Initial State: (array([-0.005, 1.419, -0.533, 0.370, 0.006, 0.121, 0.000, 0.000],\n",
      "      dtype=float32), {})\n",
      "Action: 0\n",
      "Next State: [-0.011 1.427 -0.532 0.344 0.012 0.119 0.000 0.000]\n",
      "Reward Received: 0.11857559850841426\n",
      "Episode Terminated: False\n",
      "Info: False\n",
      "F : {}\n"
     ]
    }
   ],
   "source": [
    "# Select an action\n",
    "action = 0\n",
    "\n",
    "initial_state = env.reset()\n",
    "\n",
    "# Run a single time step of the environment's dynamics with the given action.\n",
    "next_state, reward, done, info, f = env.step(action)\n",
    "#tuple = env.step(action)\n",
    "print(\"The tuple\")\n",
    "print(tuple)\n",
    "\n",
    "with np.printoptions(formatter={'float': '{:.3f}'.format}):\n",
    "    print(\"Initial State:\", initial_state)\n",
    "    print(\"Action:\", action)\n",
    "    print(\"Next State:\", next_state)\n",
    "    print(\"Reward Received:\", reward)\n",
    "    print(\"Episode Terminated:\", done)\n",
    "    print(\"Info:\", info)\n",
    "    print(\"F :\",f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0d6a291-048c-4569-af6a-b6c870821053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNQ_C1\n",
    "# # GRADED CELL\n",
    "\n",
    "# # Create the Q-Network\n",
    "# print(\"State size :\",type(state_size))\n",
    "# q_network = Sequential([\n",
    "#     ### START CODE HERE ### \n",
    "#     Input(shape=state_size),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=num_actions,activation='linear'),\n",
    "    \n",
    "#     ### END CODE HERE ### \n",
    "#     ])\n",
    "\n",
    "# # Create the target Q^-Network\n",
    "# target_q_network = Sequential([\n",
    "#     ### START CODE HERE ### \n",
    "#     Input(shape=state_size),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=num_actions,activation='linear'),\n",
    "#     ### END CODE HERE ###\n",
    "#     ])\n",
    "\n",
    "# ### START CODE HERE ### \n",
    "# optimizer = Adam(learning_rate=ALPHA)\n",
    "# ### END CODE HERE ###\n",
    "\n",
    "# print(q_network.summary(expand_nested=True))\n",
    "\n",
    "# print(target_q_network.summary())\n",
    "\n",
    "# this is how you'd presumably create a dense nn on torch using the same\n",
    "# layout as tf\n",
    "q_network = nn.Sequential(\n",
    "    nn.Linear(state_size[0], 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 4),\n",
    "    \n",
    ")\n",
    "\n",
    "target_q_network = nn.Sequential(\n",
    "    nn.Linear(state_size[0], 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 4),\n",
    "    \n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(q_network.parameters(), lr=ALPHA)\n",
    "optimizer = torch.optim.Adam(target_q_network.parameters(), lr=ALPHA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de3ca19b-c8c0-4894-ae21-9b3b15470367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 2. 1. 2. 0. 2. 3. 3. 2. 3. 3. 2. 0. 0. 0. 1. 1. 3. 0. 3. 3. 3. 2.\n",
      " 2. 1. 0. 1. 1. 2. 1. 1. 0. 0. 3. 1. 3. 0. 1. 2. 3. 1. 0. 1. 2. 2. 3. 0.\n",
      " 0. 2. 3. 0. 2. 2. 1. 3. 2. 0. 1. 3. 3. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "np.random.seed(1)\n",
    "states = np.float32(np.random.rand(64, 8))\n",
    "actions = np.float32(np.floor(np.random.uniform(0, 1, (64, )) * 4))\n",
    "rewards = np.float32(np.random.rand(64, ))\n",
    "next_states = np.float32(np.random.rand(64, 8))\n",
    "done_vals = np.float32((np.random.uniform(0, 1, size=(64,)) > 0.96) * 1)\n",
    "\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c4b5f073-ffce-43d9-88db-6336f277ebb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END\n",
      "Actions : [0. 0. 2. 1. 2. 0. 2. 3. 3. 2. 3. 3. 2. 0. 0. 0. 1. 1. 3. 0. 3. 3. 3. 2.\n",
      " 2. 1. 0. 1. 1. 2. 1. 1. 0. 0. 3. 1. 3. 0. 1. 2. 3. 1. 0. 1. 2. 2. 3. 0.\n",
      " 0. 2. 3. 0. 2. 2. 1. 3. 2. 0. 1. 3. 3. 2. 2. 2.]\n",
      "max_qsa  tensor([0.9909, 0.7804, 0.9108, 0.8211, 0.9742, 0.8894, 0.8963, 0.8731, 0.9247,\n",
      "        0.8850, 0.7176, 0.8370, 0.6384, 0.8805, 0.5914, 0.3029, 0.7417, 0.6784,\n",
      "        0.9826, 0.8164, 0.7873, 0.9588, 0.7906, 0.6890, 0.9258, 0.7467, 0.9306,\n",
      "        0.9787, 0.6606, 0.5380, 0.6963, 0.6873, 0.9796, 0.7647, 0.5348, 0.8010,\n",
      "        0.9314, 0.9192, 0.9985, 0.9718, 0.8417, 0.9110, 0.7256, 0.8847, 0.9281,\n",
      "        0.9769, 0.7104, 0.9904, 0.5269, 0.9332, 0.9595, 0.3127, 0.9537, 0.7581,\n",
      "        0.8905, 0.9713, 0.9225, 0.8797, 0.4443, 0.6910, 0.8733, 0.9841, 0.2528,\n",
      "        0.6400])\n",
      "q_values , [[0.29469204 0.2696547  0.59054655 0.38053408]\n",
      " [0.70252305 0.49612394 0.6332093  0.35339436]\n",
      " [0.28704396 0.13320711 0.9370804  0.80483586]\n",
      " [0.9380285  0.18538052 0.9331558  0.9357532 ]\n",
      " [0.2835445  0.27463424 0.5514316  0.18528923]\n",
      " [0.44210112 0.18531765 0.9473759  0.49382466]\n",
      " [0.9639178  0.6449417  0.48431578 0.96769524]\n",
      " [0.14318979 0.6323299  0.66188836 0.43383983]\n",
      " [0.05935723 0.4927226  0.46513394 0.80674684]\n",
      " [0.256661   0.39143035 0.2401722  0.6864434 ]\n",
      " [0.9898835  0.17742954 0.8214467  0.23098856]\n",
      " [0.05797581 0.13881652 0.9351531  0.8885421 ]\n",
      " [0.2892605  0.94776404 0.8499305  0.4346548 ]\n",
      " [0.46804956 0.629853   0.15803184 0.01478209]\n",
      " [0.26551202 0.01544363 0.67449886 0.55146885]\n",
      " [0.32164815 0.7802023  0.14578544 0.24488063]\n",
      " [0.29231375 0.19593194 0.08793868 0.86997616]\n",
      " [0.08818865 0.68371034 0.12040088 0.010699  ]\n",
      " [0.5572682  0.9071091  0.86045665 0.3384514 ]\n",
      " [0.02715883 0.80085933 0.8059849  0.86848813]\n",
      " [0.6696466  0.80777884 0.8974983  0.45382994]\n",
      " [0.5578253  0.5061311  0.6919839  0.68290687]\n",
      " [0.5203136  0.82846814 0.5057794  0.82485855]\n",
      " [0.2818451  0.17849483 0.96987057 0.74193954]\n",
      " [0.2597917  0.58123535 0.95526224 0.08075391]\n",
      " [0.0835342  0.61913484 0.22449347 0.765184  ]\n",
      " [0.56815255 0.66618806 0.1078139  0.08428305]\n",
      " [0.62512106 0.40973127 0.08725253 0.37106395]\n",
      " [0.6721444  0.1895874  0.8957291  0.9498464 ]\n",
      " [0.9615725  0.7300826  0.40628842 0.7437492 ]\n",
      " [0.28798687 0.3144172  0.60589635 0.59484714]\n",
      " [0.878661   0.25460076 0.7661884  0.9410133 ]\n",
      " [0.00103293 0.47608912 0.65704036 0.84332323]\n",
      " [0.18373941 0.8561592  0.3643655  0.8922739 ]\n",
      " [0.02771174 0.08539014 0.98141044 0.6095019 ]\n",
      " [0.41763234 0.16921419 0.64990115 0.03697507]\n",
      " [0.31161392 0.3234792  0.6074419  0.6508313 ]\n",
      " [0.8460546  0.92758095 0.66045773 0.57817394]\n",
      " [0.39871556 0.9666125  0.15407917 0.8950868 ]\n",
      " [0.31101307 0.5484891  0.68206626 0.23646966]\n",
      " [0.16773668 0.50848204 0.79186386 0.69241446]\n",
      " [0.11897334 0.90074223 0.47973758 0.9466767 ]\n",
      " [0.5534337  0.08480182 0.20246303 0.854238  ]\n",
      " [0.7059274  0.89493954 0.329146   0.52800137]\n",
      " [0.65039814 0.59429085 0.23146294 0.42682135]\n",
      " [0.24811146 0.84561497 0.98667747 0.0665708 ]\n",
      " [0.71392804 0.33011556 0.62543535 0.25812966]\n",
      " [0.6510076  0.3711472  0.29482087 0.9117632 ]\n",
      " [0.94249886 0.61355    0.21446778 0.93941885]\n",
      " [0.75045353 0.7510898  0.21172133 0.8301565 ]\n",
      " [0.24009515 0.63162076 0.71073955 0.7423834 ]\n",
      " [0.3902647  0.7598254  0.9029359  0.58742005]\n",
      " [0.09040972 0.12897599 0.10526253 0.17272747]\n",
      " [0.12643088 0.7783009  0.03941151 0.24155842]\n",
      " [0.8790664  0.97351885 0.605422   0.97623825]\n",
      " [0.04345154 0.94857275 0.31129608 0.79897285]\n",
      " [0.8106291  0.8599986  0.4585508  0.9306444 ]\n",
      " [0.257893   0.9344455  0.61911964 0.9854449 ]\n",
      " [0.03264387 0.7251667  0.56555426 0.49242404]\n",
      " [0.25921813 0.3268307  0.2494622  0.2192679 ]\n",
      " [0.79966086 0.50239503 0.08318989 0.7769842 ]\n",
      " [0.5336491  0.7393878  0.5153937  0.3999459 ]\n",
      " [0.98624676 0.02824262 0.93276393 0.930855  ]\n",
      " [0.3909187  0.21446487 0.6192133  0.17797233]]\n",
      "action shape  <class 'numpy.ndarray'>\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63]\n",
      "q_values 64,  [0.29469204 0.70252305 0.9370804  0.18538052 0.5514316  0.44210112\n",
      " 0.48431578 0.43383983 0.80674684 0.2401722  0.23098856 0.8885421\n",
      " 0.8499305  0.46804956 0.26551202 0.32164815 0.19593194 0.68371034\n",
      " 0.3384514  0.02715883 0.45382994 0.68290687 0.82485855 0.96987057\n",
      " 0.95526224 0.61913484 0.56815255 0.40973127 0.1895874  0.40628842\n",
      " 0.3144172  0.25460076 0.00103293 0.18373941 0.6095019  0.16921419\n",
      " 0.6508313  0.8460546  0.9666125  0.68206626 0.69241446 0.90074223\n",
      " 0.5534337  0.89493954 0.23146294 0.98667747 0.25812966 0.6510076\n",
      " 0.94249886 0.21172133 0.7423834  0.3902647  0.10526253 0.03941151\n",
      " 0.97351885 0.79897285 0.4585508  0.257893   0.7251667  0.2192679\n",
      " 0.7769842  0.5153937  0.93276393 0.6192133 ]\n",
      "Y , (64,)\n",
      "tensor(0.6992)\n"
     ]
    }
   ],
   "source": [
    "# def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "#     \"\"\" \n",
    "#     Calculates the loss.\n",
    "    \n",
    "#     Args:\n",
    "#       experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "#       gamma: (float) The discount factor.\n",
    "#       q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "#       target_q_network: (tf.keras.Sequential) Karas model for predicting the targets\n",
    "          \n",
    "#     Returns:\n",
    "#       loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "#             the y targets and the Q(s,a) values.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Unpack the mini-batch of experience tuples\n",
    "#     states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "#     # Compute max Q^(s,a)\n",
    "#     max_qsa = tf.reduce_max(target_q_network(next_states), axis=0)\n",
    "\n",
    "#     print(type(target_q_network(next_states)))\n",
    "    \n",
    "#     print(target_q_network(next_states))\n",
    "#     print(\"numpy shape \",target_q_network(next_states).shape)\n",
    "#     print(\"numpy max, \" ,np.max(target_q_network(next_states),axis=0))\n",
    "#     print(\"numpy dtype , \",target_q_network(next_states).dtype)\n",
    "#     print(max_qsa)\n",
    "    \n",
    "#     # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a).\n",
    "#     ### START CODE HERE ### \n",
    "#     y = (rewards + (1-done_vals)*(gamma*max_qsa))\n",
    "#     ### END CODE HERE ###\n",
    "    \n",
    "#     # Get the q_values\n",
    "#     q_values = q_network(states)\n",
    "#     q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "#                                                 tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "#     # Compute the loss\n",
    "#     ### START CODE HERE ### \n",
    "#     loss = MSE(y, q_values)\n",
    "#     ### END CODE HERE ### \n",
    "    \n",
    "#     return loss\n",
    "\n",
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \n",
    "    # Unpack the mini-batch of experience tuples\n",
    "    #print(experiences)\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    #input_tensor = torch.randn(20,20)\n",
    "    #print(type(input_tensor))\n",
    "    #print(input_tensor)\n",
    "\n",
    "    print(\"END\")\n",
    "    n_statae = target_q_network(next_states)\n",
    "\n",
    "    #print(n_statae)\n",
    "    max_qsa , indicies  = torch.max(torch.from_numpy(n_statae), dim=1)\n",
    "    \n",
    "    print(\"Actions :\",actions)\n",
    "\n",
    "    print(\"max_qsa \", max_qsa)\n",
    "    y = (rewards + (1-done_vals)*(gamma*max_qsa.numpy()))\n",
    "    \n",
    "    q_values = q_network(states)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"q_values ,\",q_values)\n",
    "    print(\"action shape \",type(actions))\n",
    "    print(np.arange(actions.shape[0]))\n",
    "    print(\"q_values 64, \",q_values[np.arange(actions.shape[0]),actions.astype(int)])\n",
    "    q_values = q_values[np.arange(actions.shape[0]),actions.astype(int)]\n",
    "    print(\"Y ,\",y.shape)\n",
    "    #print(\"q_values, \",q_values.size())\n",
    "    mseloss = torch.nn.MSELoss()\n",
    "    loss = mseloss(torch.from_numpy(y),torch.from_numpy(q_values))\n",
    "    #mse_loss = torch.nn.functional.mse_loss(y,q_values)\n",
    "\n",
    "    print(loss)\n",
    "    return loss\n",
    "\n",
    "public_tests.test_compute_loss(compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f62bd-1b5d-4299-b493-0e7c88615e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bda1ea-aa62-45ee-ab55-d7ae1d30b834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
