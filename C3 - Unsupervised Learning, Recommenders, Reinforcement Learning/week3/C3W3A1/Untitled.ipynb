{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4257cde4-9c87-48f9-bddc-3f1ab2bfcc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 13:31:04.734391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-31 13:31:05.612111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "#import gym\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "import random\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import public_tests\n",
    "#from tensorflow.keras import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Input\n",
    "#from tensorflow.keras.losses import MSE\n",
    "#from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26e56790-4bdd-4cbc-a6ec-c67e4e06e867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4f603f0650>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Display(visible=0, size=(840, 480)).start();\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "906efc0a-9de1-4081-85ee-67cf39017726",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "723f6ade-73d4-4622-8e0a-62069c0836d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d6c7e2f-c564-4b5c-a471-8ef7bdee9dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Shape: (8,)\n",
      "Number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "\n",
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print('State Shape:', state_size)\n",
    "print('Number of actions:', num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a2af14c-d74f-4f60-917a-95272c33da94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tuple\n",
      "<class 'tuple'>\n",
      "Initial State: (array([0.007, 1.418, 0.746, 0.334, -0.009, -0.169, 0.000, 0.000],\n",
      "      dtype=float32), {})\n",
      "Action: 0\n",
      "Next State: [0.015 1.425 0.745 0.308 -0.017 -0.167 0.000 0.000]\n",
      "Reward Received: -0.42573787541755337\n",
      "Episode Terminated: False\n",
      "Info: False\n",
      "F : {}\n"
     ]
    }
   ],
   "source": [
    "# Select an action\n",
    "action = 0\n",
    "\n",
    "initial_state = env.reset()\n",
    "\n",
    "# Run a single time step of the environment's dynamics with the given action.\n",
    "next_state, reward, done, info, f = env.step(action)\n",
    "#tuple = env.step(action)\n",
    "print(\"The tuple\")\n",
    "print(tuple)\n",
    "\n",
    "with np.printoptions(formatter={'float': '{:.3f}'.format}):\n",
    "    print(\"Initial State:\", initial_state)\n",
    "    print(\"Action:\", action)\n",
    "    print(\"Next State:\", next_state)\n",
    "    print(\"Reward Received:\", reward)\n",
    "    print(\"Episode Terminated:\", done)\n",
    "    print(\"Info:\", info)\n",
    "    print(\"F :\",f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0d6a291-048c-4569-af6a-b6c870821053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNQ_C1\n",
    "# # GRADED CELL\n",
    "\n",
    "# # Create the Q-Network\n",
    "# print(\"State size :\",type(state_size))\n",
    "# q_network = Sequential([\n",
    "#     ### START CODE HERE ### \n",
    "#     Input(shape=state_size),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=num_actions,activation='linear'),\n",
    "    \n",
    "#     ### END CODE HERE ### \n",
    "#     ])\n",
    "\n",
    "# # Create the target Q^-Network\n",
    "# target_q_network = Sequential([\n",
    "#     ### START CODE HERE ### \n",
    "#     Input(shape=state_size),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=num_actions,activation='linear'),\n",
    "#     ### END CODE HERE ###\n",
    "#     ])\n",
    "\n",
    "# ### START CODE HERE ### \n",
    "# optimizer = Adam(learning_rate=ALPHA)\n",
    "# ### END CODE HERE ###\n",
    "\n",
    "# print(q_network.summary(expand_nested=True))\n",
    "\n",
    "# print(target_q_network.summary())\n",
    "\n",
    "# this is how you'd presumably create a dense nn on torch using the same\n",
    "# layout as tf\n",
    "q_network = nn.Sequential(\n",
    "    nn.Linear(state_size[0], 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 4),\n",
    "    \n",
    ")\n",
    "\n",
    "target_q_network = nn.Sequential(\n",
    "    nn.Linear(state_size[0], 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 4),\n",
    "    \n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(q_network.parameters(), lr=ALPHA)\n",
    "optimizer = torch.optim.Adam(target_q_network.parameters(), lr=ALPHA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de3ca19b-c8c0-4894-ae21-9b3b15470367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 2. 1. 2. 0. 2. 3. 3. 2. 3. 3. 2. 0. 0. 0. 1. 1. 3. 0. 3. 3. 3. 2.\n",
      " 2. 1. 0. 1. 1. 2. 1. 1. 0. 0. 3. 1. 3. 0. 1. 2. 3. 1. 0. 1. 2. 2. 3. 0.\n",
      " 0. 2. 3. 0. 2. 2. 1. 3. 2. 0. 1. 3. 3. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "np.random.seed(1)\n",
    "states = np.float32(np.random.rand(64, 8))\n",
    "actions = np.float32(np.floor(np.random.uniform(0, 1, (64, )) * 4))\n",
    "rewards = np.float32(np.random.rand(64, ))\n",
    "next_states = np.float32(np.random.rand(64, 8))\n",
    "done_vals = np.float32((np.random.uniform(0, 1, size=(64,)) > 0.96) * 1)\n",
    "\n",
    "print(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4b5f073-ffce-43d9-88db-6336f277ebb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(array([[4.17021990e-01, 7.20324516e-01, 1.14374816e-04, 3.02332580e-01,\n",
      "        1.46755889e-01, 9.23385918e-02, 1.86260208e-01, 3.45560730e-01],\n",
      "       [3.96767467e-01, 5.38816750e-01, 4.19194520e-01, 6.85219526e-01,\n",
      "        2.04452246e-01, 8.78117442e-01, 2.73875929e-02, 6.70467496e-01],\n",
      "       [4.17304814e-01, 5.58689833e-01, 1.40386939e-01, 1.98101491e-01,\n",
      "        8.00744593e-01, 9.68261600e-01, 3.13424170e-01, 6.92322612e-01],\n",
      "       [8.76389146e-01, 8.94606650e-01, 8.50442126e-02, 3.90547849e-02,\n",
      "        1.69830427e-01, 8.78142476e-01, 9.83468369e-02, 4.21107620e-01],\n",
      "       [9.57889557e-01, 5.33165276e-01, 6.91877127e-01, 3.15515637e-01,\n",
      "        6.86500907e-01, 8.34625661e-01, 1.82882771e-02, 7.50144303e-01],\n",
      "       [9.88861084e-01, 7.48165667e-01, 2.80443996e-01, 7.89279342e-01,\n",
      "        1.03226006e-01, 4.47893530e-01, 9.08595502e-01, 2.93614149e-01],\n",
      "       [2.87775338e-01, 1.30028576e-01, 1.93669572e-02, 6.78835511e-01,\n",
      "        2.11628109e-01, 2.65546650e-01, 4.91573155e-01, 5.33625446e-02],\n",
      "       [5.74117601e-01, 1.46728575e-01, 5.89305520e-01, 6.99758351e-01,\n",
      "        1.02334432e-01, 4.14055973e-01, 6.94400132e-01, 4.14179265e-01],\n",
      "       [4.99534607e-02, 5.35896420e-01, 6.63794637e-01, 5.14889121e-01,\n",
      "        9.44594741e-01, 5.86555064e-01, 9.03401911e-01, 1.37474701e-01],\n",
      "       [1.39276341e-01, 8.07391286e-01, 3.97676826e-01, 1.65354192e-01,\n",
      "        9.27508593e-01, 3.47765863e-01, 7.50812113e-01, 7.25997984e-01],\n",
      "       [8.83306086e-01, 6.23672187e-01, 7.50942409e-01, 3.48898351e-01,\n",
      "        2.69927889e-01, 8.95886242e-01, 4.28091198e-01, 9.64840055e-01],\n",
      "       [6.63441479e-01, 6.21695697e-01, 1.14745975e-01, 9.49489236e-01,\n",
      "        4.49912131e-01, 5.78389585e-01, 4.08136815e-01, 2.37026975e-01],\n",
      "       [9.03379500e-01, 5.73679507e-01, 2.87032709e-03, 6.17144942e-01,\n",
      "        3.26644897e-01, 5.27058125e-01, 8.85942101e-01, 3.57269764e-01],\n",
      "       [9.08535123e-01, 6.23360097e-01, 1.58212427e-02, 9.29437220e-01,\n",
      "        6.90896928e-01, 9.97322857e-01, 1.72340512e-01, 1.37135744e-01],\n",
      "       [9.32595491e-01, 6.96818173e-01, 6.60001710e-02, 7.55463064e-01,\n",
      "        7.53876209e-01, 9.23024535e-01, 7.11524785e-01, 1.24270961e-01],\n",
      "       [1.98801346e-02, 2.62109861e-02, 2.83064879e-02, 2.46211067e-01,\n",
      "        8.60027969e-01, 5.38831055e-01, 5.52821994e-01, 8.42030883e-01],\n",
      "       [1.24173313e-01, 2.79183686e-01, 5.85759282e-01, 9.69595730e-01,\n",
      "        5.61030209e-01, 1.86472889e-02, 8.00632656e-01, 2.32974276e-01],\n",
      "       [8.07105184e-01, 3.87860656e-01, 8.63541842e-01, 7.47121632e-01,\n",
      "        5.56240261e-01, 1.36455223e-01, 5.99176884e-02, 1.21343456e-01],\n",
      "       [4.45518792e-02, 1.07494131e-01, 2.25709334e-01, 7.12988973e-01,\n",
      "        5.59717000e-01, 1.25559801e-02, 7.19742775e-02, 9.67276335e-01],\n",
      "       [5.68100452e-01, 2.03293234e-01, 2.52325743e-01, 7.43825853e-01,\n",
      "        1.95429474e-01, 5.81358910e-01, 9.70019996e-01, 8.46828818e-01],\n",
      "       [2.39847764e-01, 4.93769705e-01, 6.19955719e-01, 8.28980923e-01,\n",
      "        1.56791389e-01, 1.85762029e-02, 7.00221434e-02, 4.86345112e-01],\n",
      "       [6.06329441e-01, 5.68851411e-01, 3.17362398e-01, 9.88616168e-01,\n",
      "        5.79745233e-01, 3.80141169e-01, 5.50948203e-01, 7.45334446e-01],\n",
      "       [6.69232905e-01, 2.64919549e-01, 6.63348362e-02, 3.70084196e-01,\n",
      "        6.29717529e-01, 2.10174009e-01, 7.52755582e-01, 6.65364787e-02],\n",
      "       [2.60315090e-01, 8.04754555e-01, 1.93434283e-01, 6.39460862e-01,\n",
      "        5.24670303e-01, 9.24807966e-01, 2.63296783e-01, 6.59610927e-02],\n",
      "       [7.35065937e-01, 7.72178054e-01, 9.07815874e-01, 9.31972086e-01,\n",
      "        1.39515726e-02, 2.34362081e-01, 6.16778374e-01, 9.49016333e-01],\n",
      "       [9.50176120e-01, 5.56653202e-01, 9.15606380e-01, 6.41566217e-01,\n",
      "        3.90007704e-01, 4.85990673e-01, 6.04310513e-01, 5.49547911e-01],\n",
      "       [9.26181436e-01, 9.18733418e-01, 3.94875616e-01, 9.63262558e-01,\n",
      "        1.73955664e-01, 1.26329526e-01, 1.35079160e-01, 5.05662143e-01],\n",
      "       [2.15248056e-02, 9.47970212e-01, 8.27115476e-01, 1.50189810e-02,\n",
      "        1.76196262e-01, 3.32063586e-01, 1.30996838e-01, 8.09490681e-01],\n",
      "       [3.44736665e-01, 9.40107465e-01, 5.82014203e-01, 8.78831983e-01,\n",
      "        8.44734430e-01, 9.05392289e-01, 4.59880263e-01, 5.46346843e-01],\n",
      "       [7.98603594e-01, 2.85718858e-01, 4.90253508e-01, 5.99110305e-01,\n",
      "        1.55332759e-02, 5.93481421e-01, 4.33676362e-01, 8.07360530e-01],\n",
      "       [3.15244794e-01, 8.92888725e-01, 5.77857196e-01, 1.84010208e-01,\n",
      "        7.87929237e-01, 6.12031162e-01, 5.39092720e-02, 4.20193672e-01],\n",
      "       [6.79068863e-01, 9.18601751e-01, 4.02024889e-04, 9.76759136e-01,\n",
      "        3.76580328e-01, 9.73783553e-01, 6.04716122e-01, 8.28845799e-01],\n",
      "       [5.74711502e-01, 6.28076196e-01, 2.85576284e-01, 5.86833358e-01,\n",
      "        7.50021756e-01, 8.58313859e-01, 7.55082190e-01, 6.98057234e-01],\n",
      "       [8.64479423e-01, 3.22681010e-01, 6.70788765e-01, 4.50873941e-01,\n",
      "        3.82102758e-01, 4.10811365e-01, 4.01479572e-01, 3.17383945e-01],\n",
      "       [6.21919394e-01, 4.30247277e-01, 9.73802090e-01, 6.77800894e-01,\n",
      "        1.98569894e-01, 4.26701009e-01, 3.43346238e-01, 7.97638834e-01],\n",
      "       [8.79998267e-01, 9.03841972e-01, 6.62719786e-01, 2.70208269e-01,\n",
      "        2.52366692e-01, 8.54897916e-01, 5.27714670e-01, 8.02161098e-01],\n",
      "       [5.72488546e-01, 7.33142555e-01, 5.19011617e-01, 7.70883918e-01,\n",
      "        5.68857968e-01, 4.65709865e-01, 3.42688918e-01, 6.82093501e-02],\n",
      "       [3.77924174e-01, 7.96260759e-02, 9.82817113e-01, 1.81612849e-01,\n",
      "        8.11858714e-01, 8.74961674e-01, 6.88413262e-01, 5.69494426e-01],\n",
      "       [1.60971433e-01, 4.66880023e-01, 3.45172048e-01, 2.25039959e-01,\n",
      "        5.92511892e-01, 3.12269837e-01, 9.16305542e-01, 9.09635544e-01],\n",
      "       [2.57118285e-01, 1.10891297e-01, 1.92962736e-01, 4.99584168e-01,\n",
      "        7.28585660e-01, 2.08194435e-01, 2.48033553e-01, 8.51671875e-01],\n",
      "       [4.15848732e-01, 6.16685092e-01, 2.33666137e-01, 1.01967260e-01,\n",
      "        5.15857041e-01, 4.77140993e-01, 1.52671650e-01, 6.21806204e-01],\n",
      "       [5.44010103e-01, 6.54137373e-01, 1.44545540e-01, 7.51527846e-01,\n",
      "        2.22049147e-01, 5.19351840e-01, 7.85296023e-01, 2.23304275e-02],\n",
      "       [3.24362457e-01, 8.72922361e-01, 8.44709635e-01, 5.38440585e-01,\n",
      "        8.66608262e-01, 9.49805975e-01, 8.26407015e-01, 8.54115427e-01],\n",
      "       [9.87434015e-02, 6.51304305e-01, 7.03516960e-01, 6.10240817e-01,\n",
      "        7.99615264e-01, 3.45712192e-02, 7.70238757e-01, 7.31728613e-01],\n",
      "       [2.59698391e-01, 2.57069290e-01, 6.32303298e-01, 3.45297456e-01,\n",
      "        7.96588659e-01, 4.46146220e-01, 7.82749414e-01, 9.90471780e-01],\n",
      "       [3.00248325e-01, 1.43005833e-01, 9.01308417e-01, 5.41559398e-01,\n",
      "        9.74740386e-01, 6.36604428e-01, 9.93912995e-01, 5.46070814e-01],\n",
      "       [5.26425958e-01, 1.35427907e-01, 3.55705172e-01, 2.62185670e-02,\n",
      "        1.60395175e-01, 7.45637178e-01, 3.03996895e-02, 3.66543084e-01],\n",
      "       [8.62346232e-01, 6.92677736e-01, 6.90942168e-01, 1.88636795e-01,\n",
      "        4.41904277e-01, 5.81577420e-01, 9.89751697e-01, 2.03906223e-01],\n",
      "       [2.47732908e-01, 2.62173086e-01, 7.50172436e-01, 4.56975341e-01,\n",
      "        5.69294393e-02, 5.08516252e-01, 2.11960167e-01, 7.98604250e-01],\n",
      "       [2.97331393e-01, 2.76060123e-02, 5.93432426e-01, 8.43840420e-01,\n",
      "        3.81016135e-01, 7.49858320e-01, 5.11141479e-01, 5.40951788e-01],\n",
      "       [9.59434330e-01, 8.03960919e-01, 3.23230661e-02, 7.09387243e-01,\n",
      "        4.65001494e-01, 9.47548926e-01, 2.21432731e-01, 2.67072022e-01],\n",
      "       [8.14739615e-02, 4.28618819e-01, 1.09018765e-01, 6.33786738e-01,\n",
      "        8.02963257e-01, 6.96800470e-01, 7.66211390e-01, 3.42454106e-01],\n",
      "       [8.45851481e-01, 4.28768784e-01, 8.24009895e-01, 6.26496136e-01,\n",
      "        1.43423051e-01, 7.83869028e-02, 1.83326434e-02, 6.67250007e-02],\n",
      "       [4.58583802e-01, 1.13341920e-01, 2.77833492e-02, 7.54861474e-01,\n",
      "        3.94850492e-01, 7.46938467e-01, 4.52404827e-01, 4.50086743e-01],\n",
      "       [4.78072494e-01, 4.74003941e-01, 8.03163350e-01, 4.02392507e-01,\n",
      "        9.04686153e-01, 3.70610468e-02, 7.73874342e-01, 1.25641376e-01],\n",
      "       [6.18513584e-01, 1.03642615e-02, 5.38627267e-01, 3.01795662e-03,\n",
      "        9.51193810e-01, 9.05402005e-01, 7.95966923e-01, 9.15274322e-01],\n",
      "       [1.45558238e-01, 1.57730073e-01, 1.87631667e-01, 6.22495890e-01,\n",
      "        9.05809522e-01, 9.89955187e-01, 7.11122453e-01, 7.31800437e-01],\n",
      "       [9.09293175e-01, 4.00873721e-01, 2.49850675e-01, 1.73430175e-01,\n",
      "        1.19457051e-01, 8.12610567e-01, 1.46792367e-01, 2.64297485e-01],\n",
      "       [8.19089174e-01, 3.10587257e-01, 9.82417464e-01, 2.66638696e-01,\n",
      "        5.33653319e-01, 3.14467013e-01, 9.10772860e-01, 3.66556644e-01],\n",
      "       [4.33592319e-01, 5.12292683e-01, 9.38886464e-01, 3.09490059e-02,\n",
      "        7.16878653e-01, 8.91018927e-01, 2.72872243e-02, 5.22051275e-01],\n",
      "       [3.25989813e-01, 8.59489322e-01, 5.58516562e-01, 6.90227866e-01,\n",
      "        4.52853501e-01, 6.28309011e-01, 2.90096849e-01, 9.34857782e-03],\n",
      "       [5.76755941e-01, 3.11444223e-01, 5.17267585e-01, 9.16405857e-01,\n",
      "        4.26474780e-01, 2.47396037e-01, 3.71293753e-01, 9.31861103e-01],\n",
      "       [9.36868370e-01, 8.44329953e-01, 9.20206487e-01, 2.27900296e-01,\n",
      "        8.74822065e-02, 2.27309734e-01, 3.14376622e-01, 1.74765870e-01],\n",
      "       [6.07094169e-01, 4.13586408e-01, 8.16351533e-01, 1.85130402e-01,\n",
      "        7.01876521e-01, 2.40355626e-01, 5.74219108e-01, 3.48987609e-01]],\n",
      "      dtype=float32), array([0., 0., 2., 1., 2., 0., 2., 3., 3., 2., 3., 3., 2., 0., 0., 0., 1.,\n",
      "       1., 3., 0., 3., 3., 3., 2., 2., 1., 0., 1., 1., 2., 1., 1., 0., 0.,\n",
      "       3., 1., 3., 0., 1., 2., 3., 1., 0., 1., 2., 2., 3., 0., 0., 2., 3.,\n",
      "       0., 2., 2., 1., 3., 2., 0., 1., 3., 3., 2., 2., 2.], dtype=float32), array([0.27411976, 0.56053   , 0.6717298 , 0.35242963, 0.85582834,\n",
      "       0.19503748, 0.74732083, 0.28960276, 0.7737993 , 0.42773733,\n",
      "       0.8076984 , 0.35353488, 0.21369323, 0.7672845 , 0.30864194,\n",
      "       0.7332451 , 0.74447316, 0.2213967 , 0.21411213, 0.19894792,\n",
      "       0.14251834, 0.3770826 , 0.02662789, 0.11092037, 0.674564  ,\n",
      "       0.79977655, 0.08052953, 0.23170231, 0.20762566, 0.91733354,\n",
      "       0.7113145 , 0.5538846 , 0.30451798, 0.83485407, 0.43530595,\n",
      "       0.9234562 , 0.7060518 , 0.4780313 , 0.12621011, 0.9760435 ,\n",
      "       0.15983365, 0.20260212, 0.43118176, 0.4042019 , 0.14675148,\n",
      "       0.7293189 , 0.18874507, 0.6438956 , 0.75430596, 0.21073239,\n",
      "       0.60095423, 0.74892837, 0.6382187 , 0.5971273 , 0.29548228,\n",
      "       0.7316065 , 0.94530845, 0.4255614 , 0.7821818 , 0.05614104,\n",
      "       0.8352716 , 0.19225001, 0.39509687, 0.30008104], dtype=float32), array([[0.08010364, 0.904631  , 0.37015417, 0.53069746, 0.49411628,\n",
      "        0.13216114, 0.20645405, 0.07618881],\n",
      "       [0.5079217 , 0.26154956, 0.3570616 , 0.10806533, 0.7875518 ,\n",
      "        0.10658388, 0.98570883, 0.17716116],\n",
      "       [0.5724051 , 0.04484534, 0.7871163 , 0.18960595, 0.527904  ,\n",
      "        0.74007756, 0.14993149, 0.5510872 ],\n",
      "       [0.21661721, 0.75919604, 0.7229152 , 0.17654903, 0.86196655,\n",
      "        0.0197751 , 0.860237  , 0.5589038 ],\n",
      "       [0.40322047, 0.7587469 , 0.716929  , 0.98732615, 0.27808505,\n",
      "        0.00379367, 0.9339026 , 0.8578971 ],\n",
      "       [0.7288509 , 0.51668876, 0.70695627, 0.78052956, 0.37487593,\n",
      "        0.7703225 , 0.7506243 , 0.6132112 ],\n",
      "       [0.40186593, 0.697308  , 0.00311286, 0.7748966 , 0.8964166 ,\n",
      "        0.2393157 , 0.12076718, 0.22028399],\n",
      "       [0.30209672, 0.8830285 , 0.54316646, 0.28671166, 0.13835469,\n",
      "        0.29014447, 0.6138711 , 0.32413852],\n",
      "       [0.45736018, 0.4441171 , 0.8281354 , 0.42634815, 0.34569883,\n",
      "        0.6749716 , 0.22148205, 0.46724582],\n",
      "       [0.3147657 , 0.6268556 , 0.87736046, 0.447689  , 0.78445745,\n",
      "        0.4569657 , 0.6562293 , 0.13184097],\n",
      "       [0.43298152, 0.909312  , 0.605479  , 0.7667746 , 0.5047006 ,\n",
      "        0.49805564, 0.84289986, 0.06780694],\n",
      "       [0.5732723 , 0.94276255, 0.51786005, 0.19446582, 0.8479394 ,\n",
      "        0.25163913, 0.70072603, 0.5402609 ],\n",
      "       [0.94883627, 0.6243367 , 0.83797795, 0.00793288, 0.98934007,\n",
      "        0.07771457, 0.32212952, 0.9461524 ],\n",
      "       [0.0089391 , 0.82273   , 0.86121166, 0.4398308 , 0.2557452 ,\n",
      "        0.8026895 , 0.477862  , 0.1343386 ],\n",
      "       [0.927849  , 0.89597   , 0.49154514, 0.8567025 , 0.41857803,\n",
      "        0.6834649 , 0.39799064, 0.505742  ],\n",
      "       [0.1895517 , 0.96498895, 0.29421568, 0.10345956, 0.1443154 ,\n",
      "        0.01409229, 0.7159457 , 0.5644983 ],\n",
      "       [0.7945784 , 0.5070799 , 0.79182106, 0.69576424, 0.7778485 ,\n",
      "        0.40648288, 0.64777064, 0.1797943 ],\n",
      "       [0.32181996, 0.17260462, 0.40863723, 0.24141875, 0.40692198,\n",
      "        0.97522235, 0.32031932, 0.98249096],\n",
      "       [0.6363061 , 0.37509102, 0.8574845 , 0.61958677, 0.25203308,\n",
      "        0.7928557 , 0.43293852, 0.35751116],\n",
      "       [0.33027694, 0.69736886, 0.2686501 , 0.808278  , 0.2952888 ,\n",
      "        0.5441214 , 0.4879215 , 0.8553564 ],\n",
      "       [0.8883864 , 0.18438444, 0.5853485 , 0.89820504, 0.44611722,\n",
      "        0.9218683 , 0.2789909 , 0.60883117],\n",
      "       [0.6824537 , 0.22820574, 0.01376751, 0.41672397, 0.93848187,\n",
      "        0.3430281 , 0.77974427, 0.1747363 ],\n",
      "       [0.34195283, 0.14459772, 0.7167708 , 0.6993076 , 0.6884973 ,\n",
      "        0.25339603, 0.6923601 , 0.22729754],\n",
      "       [0.42464912, 0.37192214, 0.3553079 , 0.05765481, 0.63164663,\n",
      "        0.7073166 , 0.6135887 , 0.64831275],\n",
      "       [0.16994071, 0.1494468 , 0.51417506, 0.8753327 , 0.18395343,\n",
      "        0.46283913, 0.42893234, 0.49728918],\n",
      "       [0.16151077, 0.34244063, 0.2618804 , 0.84452695, 0.80033225,\n",
      "        0.42663917, 0.6070155 , 0.14546561],\n",
      "       [0.5096133 , 0.2969471 , 0.85965097, 0.6715984 , 0.633474  ,\n",
      "        0.1247513 , 0.47058788, 0.9865728 ],\n",
      "       [0.94829917, 0.64508563, 0.15172487, 0.63912684, 0.5656621 ,\n",
      "        0.46866584, 0.42803746, 0.59926975],\n",
      "       [0.84996986, 0.75112104, 0.57936054, 0.9247042 , 0.06473998,\n",
      "        0.99134654, 0.05299454, 0.19949554],\n",
      "       [0.42275265, 0.10750888, 0.6236704 , 0.04799256, 0.2846239 ,\n",
      "        0.06103668, 0.70351934, 0.6684562 ],\n",
      "       [0.3785806 , 0.18819426, 0.7470048 , 0.3403793 , 0.79530114,\n",
      "        0.48790094, 0.52566946, 0.02849085],\n",
      "       [0.64423203, 0.3506565 , 0.22920503, 0.43388337, 0.38246745,\n",
      "        0.46978903, 0.97948337, 0.36437806],\n",
      "       [0.7744101 , 0.5527676 , 0.88913107, 0.35495284, 0.24551868,\n",
      "        0.91101927, 0.04353426, 0.95075345],\n",
      "       [0.5564069 , 0.37636322, 0.99505234, 0.05836265, 0.51670635,\n",
      "        0.03109707, 0.57117575, 0.18046851],\n",
      "       [0.6309592 , 0.98092365, 0.87490255, 0.45183626, 0.70846087,\n",
      "        0.77746874, 0.4948431 , 0.5285335 ],\n",
      "       [0.1507844 , 0.36939994, 0.14222126, 0.7268938 , 0.477013  ,\n",
      "        0.44887882, 0.88599795, 0.52761877],\n",
      "       [0.40909085, 0.26889202, 0.07201204, 0.41813612, 0.02575348,\n",
      "        0.29115394, 0.5035095 , 0.9659331 ],\n",
      "       [0.10938291, 0.67304105, 0.49993238, 0.77709824, 0.14360699,\n",
      "        0.08320264, 0.39921862, 0.79696226],\n",
      "       [0.19167574, 0.7677772 , 0.29029799, 0.21689148, 0.01671559,\n",
      "        0.39865905, 0.3810815 , 0.6593449 ],\n",
      "       [0.0709184 , 0.15260398, 0.01657588, 0.11379635, 0.65178925,\n",
      "        0.40265685, 0.32102633, 0.557912  ],\n",
      "       [0.9934605 , 0.8344865 , 0.69962317, 0.9182586 , 0.0397287 ,\n",
      "        0.07033344, 0.4740063 , 0.3491674 ],\n",
      "       [0.93725204, 0.48956496, 0.5396491 , 0.8952604 , 0.44663504,\n",
      "        0.87703437, 0.25358176, 0.2738097 ],\n",
      "       [0.3283614 , 0.54756427, 0.22012867, 0.67142916, 0.14279328,\n",
      "        0.09410027, 0.87019175, 0.23686871],\n",
      "       [0.386004  , 0.5715421 , 0.52580196, 0.07602388, 0.87412596,\n",
      "        0.95113564, 0.81250733, 0.28380182],\n",
      "       [0.5278468 , 0.3394167 , 0.5546673 , 0.97440344, 0.3117029 ,\n",
      "        0.6687966 , 0.3259672 , 0.77447724],\n",
      "       [0.32580996, 0.8898274 , 0.75170773, 0.7626321 , 0.46947902,\n",
      "        0.2107645 , 0.04147508, 0.3218288 ],\n",
      "       [0.03711266, 0.6938554 , 0.67035   , 0.43047178, 0.767789  ,\n",
      "        0.5360085 , 0.03985993, 0.13479312],\n",
      "       [0.1934164 , 0.3356638 , 0.05231295, 0.6051168 , 0.512061  ,\n",
      "        0.617461  , 0.43235558, 0.8477005 ],\n",
      "       [0.45405906, 0.01540352, 0.87306815, 0.65620154, 0.82300305,\n",
      "        0.95177567, 0.05091238, 0.23507187],\n",
      "       [0.06334344, 0.4216579 , 0.86382914, 0.08162399, 0.47311196,\n",
      "        0.1255431 , 0.7728856 , 0.84142214],\n",
      "       [0.04329094, 0.48644075, 0.23941104, 0.95247376, 0.9438926 ,\n",
      "        0.613934  , 0.9734874 , 0.34486136],\n",
      "       [0.8978507 , 0.43459496, 0.23581463, 0.94082797, 0.68421805,\n",
      "        0.06491159, 0.87042487, 0.7013803 ],\n",
      "       [0.6049269 , 0.7323749 , 0.25343904, 0.60048896, 0.8146192 ,\n",
      "        0.05411366, 0.13051067, 0.8424458 ],\n",
      "       [0.61834586, 0.5312878 , 0.24829073, 0.29507858, 0.87268615,\n",
      "        0.42166594, 0.06443026, 0.89698505],\n",
      "       [0.20338084, 0.82622755, 0.8817706 , 0.48675075, 0.59846467,\n",
      "        0.52726746, 0.6248213 , 0.85504174],\n",
      "       [0.28213945, 0.8837555 , 0.5676903 , 0.11510304, 0.22700116,\n",
      "        0.5959824 , 0.23944624, 0.13141567],\n",
      "       [0.16184771, 0.84487253, 0.60218364, 0.9635671 , 0.34567922,\n",
      "        0.5956251 , 0.5989853 , 0.6157043 ],\n",
      "       [0.05917721, 0.75031716, 0.9482095 , 0.53467906, 0.19255602,\n",
      "        0.75292593, 0.00731897, 0.3282561 ],\n",
      "       [0.91760635, 0.58836734, 0.8551903 , 0.6046721 , 0.82257813,\n",
      "        0.8794835 , 0.32098636, 0.12295477],\n",
      "       [0.72130334, 0.44034708, 0.1267357 , 0.58982366, 0.03606831,\n",
      "        0.20018211, 0.78830117, 0.01209676],\n",
      "       [0.30334568, 0.02137596, 0.997485  , 0.58203   , 0.29337627,\n",
      "        0.9289495 , 0.5071194 , 0.4546922 ],\n",
      "       [0.5878715 , 0.26413766, 0.30528829, 0.37165457, 0.244448  ,\n",
      "        0.5845473 , 0.69584614, 0.07194722],\n",
      "       [0.97108334, 0.75307065, 0.8061632 , 0.7516404 , 0.08006097,\n",
      "        0.48169628, 0.44567296, 0.67247266],\n",
      "       [0.44873726, 0.7043134 , 0.68164533, 0.6971485 , 0.6186007 ,\n",
      "        0.15098073, 0.76080245, 0.7810458 ]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32))\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "#     \"\"\" \n",
    "#     Calculates the loss.\n",
    "    \n",
    "#     Args:\n",
    "#       experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "#       gamma: (float) The discount factor.\n",
    "#       q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "#       target_q_network: (tf.keras.Sequential) Karas model for predicting the targets\n",
    "          \n",
    "#     Returns:\n",
    "#       loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "#             the y targets and the Q(s,a) values.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Unpack the mini-batch of experience tuples\n",
    "#     states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "#     # Compute max Q^(s,a)\n",
    "#     max_qsa = tf.reduce_max(target_q_network(next_states), axis=0)\n",
    "\n",
    "#     print(type(target_q_network(next_states)))\n",
    "    \n",
    "#     print(target_q_network(next_states))\n",
    "#     print(\"numpy shape \",target_q_network(next_states).shape)\n",
    "#     print(\"numpy max, \" ,np.max(target_q_network(next_states),axis=0))\n",
    "#     print(\"numpy dtype , \",target_q_network(next_states).dtype)\n",
    "#     print(max_qsa)\n",
    "    \n",
    "#     # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a).\n",
    "#     ### START CODE HERE ### \n",
    "#     y = (rewards + (1-done_vals)*(gamma*max_qsa))\n",
    "#     ### END CODE HERE ###\n",
    "    \n",
    "#     # Get the q_values\n",
    "#     q_values = q_network(states)\n",
    "#     q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "#                                                 tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "#     # Compute the loss\n",
    "#     ### START CODE HERE ### \n",
    "#     loss = MSE(y, q_values)\n",
    "#     ### END CODE HERE ### \n",
    "    \n",
    "#     return loss\n",
    "\n",
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \n",
    "    # Unpack the mini-batch of experience tuples\n",
    "    \n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    print(type(states))\n",
    "    print(type(actions))\n",
    "    print(experiences)\n",
    "    #input_tensor = torch.randn(20,20)\n",
    "    #print(type(input_tensor))\n",
    "    #print(input_tensor)\n",
    "    print(type(states))\n",
    "    print(type(rewards))\n",
    "    #print(\"END\")\n",
    "    n_statae = target_q_network(next_states)\n",
    "\n",
    "    #print(n_statae)\n",
    "    max_qsa , indicies  = torch.max(torch.from_numpy(n_statae), dim=1)\n",
    "    \n",
    "    #print(\"Actions :\",actions)\n",
    "\n",
    "    #print(\"max_qsa \", max_qsa)\n",
    "    y = (rewards + (1-done_vals)*(gamma*max_qsa.numpy()))\n",
    "    \n",
    "    q_values = q_network(states)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #print(\"q_values ,\",q_values)\n",
    "    #print(\"action shape \",type(actions))\n",
    "    #print(np.arange(actions.shape[0]))\n",
    "    #print(\"q_values 64, \",q_values[np.arange(actions.shape[0]),actions.astype(int)])\n",
    "    q_values = q_values[np.arange(actions.shape[0]),actions.astype(int)]\n",
    "    #print(\"Y ,\",y.shape)\n",
    "    #print(\"q_values, \",q_values.size())\n",
    "    mseloss = torch.nn.MSELoss()\n",
    "    loss = mseloss(torch.from_numpy(y),torch.from_numpy(q_values))\n",
    "    #mse_loss = torch.nn.functional.mse_loss(y,q_values)\n",
    "\n",
    "    #print(loss)\n",
    "    return loss\n",
    "\n",
    "public_tests.test_compute_loss(compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b83f62bd-1b5d-4299-b493-0e7c88615e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25bda1ea-aa62-45ee-ab55-d7ae1d30b834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "<generator object <genexpr> at 0x7f4ebf333d10>\n",
      "yoooooo we here aya\n",
      "yoooo this aint it\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "SEED = 0              # seed for pseudo-random number generator\n",
    "MINIBATCH_SIZE = 64   # mini-batch size\n",
    "TAU = 1e-3            # soft update parameter\n",
    "E_DECAY = 0.995       # ε decay rate for ε-greedy policy\n",
    "E_MIN = 0.01          # minimum ε value for ε-greedy policy\n",
    "\n",
    "num_episodes = 2000\n",
    "max_num_timesteps = 1000\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100    # number of total points to use for averaging\n",
    "epsilon = 1.0     # initial ε value for ε-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "weights = q_network.state_dict()\n",
    "target_q_network.load_state_dict(weights)\n",
    "\n",
    "for source_param, target_param in zip(q_network.parameters(), target_q_network.parameters()):\n",
    "    assert torch.all(torch.eq(source_param, target_param))\n",
    "#print(weights)\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_points = 0\n",
    "    \n",
    "    for j in range(max_num_timesteps):\n",
    "    \n",
    "        actions = q_network(torch.from_numpy(state[0]))\n",
    "\n",
    "        #print(\"Theser are the actions: \",actions)\n",
    "        #print(actions)\n",
    "        #print(hello)\n",
    "        #e-greedy policy\n",
    "        if random.random() > epsilon:\n",
    "            new_action=torch.argmax(actions)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            new_action=torch.argmax(actions)\n",
    "            \n",
    "\n",
    "        epsilon = max(E_MIN, E_DECAY*epsilon)\n",
    "        new_states,reward,done,_,_ = env.step(new_action.item())\n",
    "        \n",
    "        memory_buffer.append(experience(state,new_action,reward,new_states,done))\n",
    "\n",
    "       \n",
    "      \n",
    "       \n",
    "\n",
    "        if (j + 1)%4 == 0 and len(memory_buffer) > 64 :\n",
    "            experiences = random.sample(memory_buffer, k=64)\n",
    "\n",
    "            #states = tf.convert_to_tensor(np.array([e.state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "            print([e.action for e in experiences])\n",
    "            #print(experiences)\n",
    "            print(test_states)\n",
    "            print(\"yoooooo we here aya\")\n",
    "            #print(np.array([e.state for e in experiences if e is not None]))\n",
    "            #print(experiences)\n",
    "            print(\"yoooo this aint it\")\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            break\n",
    "    break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "74c14147-8df7-4a26-bfe2-7043e8eded15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([3, 6])\n",
      "tensor([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# find the maximum value and index of the tensor along the second dimension\n",
    "max_val, max_idx = torch.max(x, dim=1)\n",
    "\n",
    "print(x)\n",
    "print(max_val)\n",
    "print(max_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c7811956-0d31-4a7d-a5b4-57156566ad6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "8\n",
      "12\n",
      "16\n",
      "20\n",
      "24\n",
      "28\n",
      "32\n",
      "36\n",
      "40\n",
      "44\n",
      "48\n",
      "52\n",
      "56\n",
      "60\n",
      "64\n",
      "68\n",
      "72\n",
      "76\n",
      "80\n",
      "84\n",
      "88\n",
      "92\n",
      "96\n",
      "100\n",
      "104\n",
      "108\n",
      "112\n",
      "116\n",
      "120\n",
      "124\n",
      "128\n",
      "132\n",
      "136\n",
      "140\n",
      "144\n",
      "148\n",
      "152\n",
      "156\n",
      "160\n",
      "164\n",
      "168\n",
      "172\n",
      "176\n",
      "180\n",
      "184\n",
      "188\n",
      "192\n",
      "196\n",
      "200\n",
      "204\n",
      "208\n",
      "212\n",
      "216\n",
      "220\n",
      "224\n",
      "228\n",
      "232\n",
      "236\n",
      "240\n",
      "244\n",
      "248\n",
      "252\n",
      "256\n",
      "260\n",
      "264\n",
      "268\n",
      "272\n",
      "276\n",
      "280\n",
      "284\n",
      "288\n",
      "292\n",
      "296\n",
      "300\n",
      "304\n",
      "308\n",
      "312\n",
      "316\n",
      "320\n",
      "324\n",
      "328\n",
      "332\n",
      "336\n",
      "340\n",
      "344\n",
      "348\n",
      "352\n",
      "356\n",
      "360\n",
      "364\n",
      "368\n",
      "372\n",
      "376\n",
      "380\n",
      "384\n",
      "388\n",
      "392\n",
      "396\n",
      "400\n",
      "404\n",
      "408\n",
      "412\n",
      "416\n",
      "420\n",
      "424\n",
      "428\n",
      "432\n",
      "436\n",
      "440\n",
      "444\n",
      "448\n",
      "452\n",
      "456\n",
      "460\n",
      "464\n",
      "468\n",
      "472\n",
      "476\n",
      "480\n",
      "484\n",
      "488\n",
      "492\n",
      "496\n",
      "500\n",
      "504\n",
      "508\n",
      "512\n",
      "516\n",
      "520\n",
      "524\n",
      "528\n",
      "532\n",
      "536\n",
      "540\n",
      "544\n",
      "548\n",
      "552\n",
      "556\n",
      "560\n",
      "564\n",
      "568\n",
      "572\n",
      "576\n",
      "580\n",
      "584\n",
      "588\n",
      "592\n",
      "596\n",
      "600\n",
      "604\n",
      "608\n",
      "612\n",
      "616\n",
      "620\n",
      "624\n",
      "628\n",
      "632\n",
      "636\n",
      "640\n",
      "644\n",
      "648\n",
      "652\n",
      "656\n",
      "660\n",
      "664\n",
      "668\n",
      "672\n",
      "676\n",
      "680\n",
      "684\n",
      "688\n",
      "692\n",
      "696\n",
      "700\n",
      "704\n",
      "708\n",
      "712\n",
      "716\n",
      "720\n",
      "724\n",
      "728\n",
      "732\n",
      "736\n",
      "740\n",
      "744\n",
      "748\n",
      "752\n",
      "756\n",
      "760\n",
      "764\n",
      "768\n",
      "772\n",
      "776\n",
      "780\n",
      "784\n",
      "788\n",
      "792\n",
      "796\n",
      "800\n",
      "804\n",
      "808\n",
      "812\n",
      "816\n",
      "820\n",
      "824\n",
      "828\n",
      "832\n",
      "836\n",
      "840\n",
      "844\n",
      "848\n",
      "852\n",
      "856\n",
      "860\n",
      "864\n",
      "868\n",
      "872\n",
      "876\n",
      "880\n",
      "884\n",
      "888\n",
      "892\n",
      "896\n",
      "900\n",
      "904\n",
      "908\n",
      "912\n",
      "916\n",
      "920\n",
      "924\n",
      "928\n",
      "932\n",
      "936\n",
      "940\n",
      "944\n",
      "948\n",
      "952\n",
      "956\n",
      "960\n",
      "964\n",
      "968\n",
      "972\n",
      "976\n",
      "980\n",
      "984\n",
      "988\n",
      "992\n",
      "996\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for i in range(max_num_timesteps):\n",
    "    counter = counter + 1\n",
    "    if (i + 1) % 4 == 0:\n",
    "        print(counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8336101-def3-48ef-8033-2a17f92d2ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
