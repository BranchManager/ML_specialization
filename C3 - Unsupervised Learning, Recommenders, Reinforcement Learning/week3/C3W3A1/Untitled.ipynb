{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4257cde4-9c87-48f9-bddc-3f1ab2bfcc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "#import gym\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "import random\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import public_tests\n",
    "#from tensorflow.keras import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Input\n",
    "#from tensorflow.keras.losses import MSE\n",
    "#from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "26e56790-4bdd-4cbc-a6ec-c67e4e06e867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f166c730990>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Display(visible=0, size=(840, 480)).start();\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "906efc0a-9de1-4081-85ee-67cf39017726",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "723f6ade-73d4-4622-8e0a-62069c0836d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7d6c7e2f-c564-4b5c-a471-8ef7bdee9dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Shape: (8,)\n",
      "Number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "\n",
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print('State Shape:', state_size)\n",
    "print('Number of actions:', num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1a2af14c-d74f-4f60-917a-95272c33da94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tuple\n",
      "<class 'tuple'>\n",
      "Initial State: (array([-0.004, 1.419, -0.435, 0.365, 0.005, 0.099, 0.000, 0.000],\n",
      "      dtype=float32), {})\n",
      "Action: 0\n",
      "Next State: [-0.009 1.427 -0.435 0.339 0.010 0.097 0.000 0.000]\n",
      "Reward Received: 0.41100255639230454\n",
      "Episode Terminated: False\n",
      "Info: False\n",
      "F : {}\n"
     ]
    }
   ],
   "source": [
    "# Select an action\n",
    "action = 0\n",
    "\n",
    "initial_state = env.reset()\n",
    "\n",
    "# Run a single time step of the environment's dynamics with the given action.\n",
    "next_state, reward, done, info, f = env.step(action)\n",
    "#tuple = env.step(action)\n",
    "print(\"The tuple\")\n",
    "print(tuple)\n",
    "\n",
    "with np.printoptions(formatter={'float': '{:.3f}'.format}):\n",
    "    print(\"Initial State:\", initial_state)\n",
    "    print(\"Action:\", action)\n",
    "    print(\"Next State:\", next_state)\n",
    "    print(\"Reward Received:\", reward)\n",
    "    print(\"Episode Terminated:\", done)\n",
    "    print(\"Info:\", info)\n",
    "    print(\"F :\",f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a0d6a291-048c-4569-af6a-b6c870821053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNQ_C1\n",
    "# # GRADED CELL\n",
    "\n",
    "# # Create the Q-Network\n",
    "# print(\"State size :\",type(state_size))\n",
    "# q_network = Sequential([\n",
    "#     ### START CODE HERE ### \n",
    "#     Input(shape=state_size),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=num_actions,activation='linear'),\n",
    "    \n",
    "#     ### END CODE HERE ### \n",
    "#     ])\n",
    "\n",
    "# # Create the target Q^-Network\n",
    "# target_q_network = Sequential([\n",
    "#     ### START CODE HERE ### \n",
    "#     Input(shape=state_size),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=64,activation='relu'),\n",
    "#     Dense(units=num_actions,activation='linear'),\n",
    "#     ### END CODE HERE ###\n",
    "#     ])\n",
    "\n",
    "# ### START CODE HERE ### \n",
    "# optimizer = Adam(learning_rate=ALPHA)\n",
    "# ### END CODE HERE ###\n",
    "\n",
    "# print(q_network.summary(expand_nested=True))\n",
    "\n",
    "# print(target_q_network.summary())\n",
    "\n",
    "# this is how you'd presumably create a dense nn on torch using the same\n",
    "# layout as tf\n",
    "q_network = nn.Sequential(\n",
    "    nn.Linear(state_size[0], 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 4),\n",
    "    \n",
    ")\n",
    "\n",
    "target_q_network = nn.Sequential(\n",
    "    nn.Linear(state_size[0], 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 4),\n",
    "    \n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(q_network.parameters(), lr=ALPHA)\n",
    "optimizer = torch.optim.Adam(target_q_network.parameters(), lr=ALPHA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "de3ca19b-c8c0-4894-ae21-9b3b15470367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 2. 1. 2. 0. 2. 3. 3. 2. 3. 3. 2. 0. 0. 0. 1. 1. 3. 0. 3. 3. 3. 2.\n",
      " 2. 1. 0. 1. 1. 2. 1. 1. 0. 0. 3. 1. 3. 0. 1. 2. 3. 1. 0. 1. 2. 2. 3. 0.\n",
      " 0. 2. 3. 0. 2. 2. 1. 3. 2. 0. 1. 3. 3. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "np.random.seed(1)\n",
    "states = np.float32(np.random.rand(64, 8))\n",
    "actions = np.float32(np.floor(np.random.uniform(0, 1, (64, )) * 4))\n",
    "rewards = np.float32(np.random.rand(64, ))\n",
    "next_states = np.float32(np.random.rand(64, 8))\n",
    "done_vals = np.float32((np.random.uniform(0, 1, size=(64,)) > 0.96) * 1)\n",
    "\n",
    "print(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c4b5f073-ffce-43d9-88db-6336f277ebb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "#     \"\"\" \n",
    "#     Calculates the loss.\n",
    "    \n",
    "#     Args:\n",
    "#       experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "#       gamma: (float) The discount factor.\n",
    "#       q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "#       target_q_network: (tf.keras.Sequential) Karas model for predicting the targets\n",
    "          \n",
    "#     Returns:\n",
    "#       loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "#             the y targets and the Q(s,a) values.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Unpack the mini-batch of experience tuples\n",
    "#     states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "#     # Compute max Q^(s,a)\n",
    "#     max_qsa = tf.reduce_max(target_q_network(next_states), axis=0)\n",
    "\n",
    "#     print(type(target_q_network(next_states)))\n",
    "    \n",
    "#     print(target_q_network(next_states))\n",
    "#     print(\"numpy shape \",target_q_network(next_states).shape)\n",
    "#     print(\"numpy max, \" ,np.max(target_q_network(next_states),axis=0))\n",
    "#     print(\"numpy dtype , \",target_q_network(next_states).dtype)\n",
    "#     print(max_qsa)\n",
    "    \n",
    "#     # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a).\n",
    "#     ### START CODE HERE ### \n",
    "#     y = (rewards + (1-done_vals)*(gamma*max_qsa))\n",
    "#     ### END CODE HERE ###\n",
    "    \n",
    "#     # Get the q_values\n",
    "#     q_values = q_network(states)\n",
    "#     q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "#                                                 tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "#     # Compute the loss\n",
    "#     ### START CODE HERE ### \n",
    "#     loss = MSE(y, q_values)\n",
    "#     ### END CODE HERE ### \n",
    "    \n",
    "#     return loss\n",
    "\n",
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \n",
    "    # Unpack the mini-batch of experience tuples\n",
    "  \n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    #input_tensor = torch.randn(20,20)\n",
    "    #print(type(input_tensor))\n",
    "    #print(input_tensor)\n",
    "    print(type(states))\n",
    "    print(type(rewards))\n",
    "    #print(\"END\")\n",
    "    n_statae = target_q_network(next_states)\n",
    "\n",
    "    #print(n_statae)\n",
    "    max_qsa , indicies  = torch.max(torch.from_numpy(n_statae), dim=1)\n",
    "    \n",
    "    #print(\"Actions :\",actions)\n",
    "\n",
    "    #print(\"max_qsa \", max_qsa)\n",
    "    y = (rewards + (1-done_vals)*(gamma*max_qsa.numpy()))\n",
    "    \n",
    "    q_values = q_network(states)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #print(\"q_values ,\",q_values)\n",
    "    #print(\"action shape \",type(actions))\n",
    "    #print(np.arange(actions.shape[0]))\n",
    "    #print(\"q_values 64, \",q_values[np.arange(actions.shape[0]),actions.astype(int)])\n",
    "    q_values = q_values[np.arange(actions.shape[0]),actions.astype(int)]\n",
    "    #print(\"Y ,\",y.shape)\n",
    "    #print(\"q_values, \",q_values.size())\n",
    "    mseloss = torch.nn.MSELoss()\n",
    "    loss = mseloss(torch.from_numpy(y),torch.from_numpy(q_values))\n",
    "    #mse_loss = torch.nn.functional.mse_loss(y,q_values)\n",
    "\n",
    "    #print(loss)\n",
    "    return loss\n",
    "\n",
    "public_tests.test_compute_loss(compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b83f62bd-1b5d-4299-b493-0e7c88615e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "25bda1ea-aa62-45ee-ab55-d7ae1d30b834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0219,  0.1630,  0.1103, -0.0180], grad_fn=<ViewBackward0>)\n",
      "(array([-0.01112967,  1.4000072 , -0.5691295 , -0.25517553,  0.01476436,\n",
      "        0.16717382,  0.        ,  0.        ], dtype=float32), -2.1840408573874286, False, False, {})\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m new_states \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(new_action\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_states)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mval\u001b[49m)        \n",
      "\u001b[0;31mNameError\u001b[0m: name 'val' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "SEED = 0              # seed for pseudo-random number generator\n",
    "MINIBATCH_SIZE = 64   # mini-batch size\n",
    "TAU = 1e-3            # soft update parameter\n",
    "E_DECAY = 0.995       # ε decay rate for ε-greedy policy\n",
    "E_MIN = 0.01          # minimum ε value for ε-greedy policy\n",
    "\n",
    "num_episodes = 2000\n",
    "max_num_timesteps = 1000\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100    # number of total points to use for averaging\n",
    "epsilon = 1.0     # initial ε value for ε-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "weights = q_network.state_dict()\n",
    "target_q_network.load_state_dict(weights)\n",
    "\n",
    "for source_param, target_param in zip(q_network.parameters(), target_q_network.parameters()):\n",
    "    assert torch.all(torch.eq(source_param, target_param))\n",
    "#print(weights)\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_points = 0\n",
    "    \n",
    "    for j in range(max_num_timesteps):\n",
    "    \n",
    "        actions = q_network(torch.from_numpy(state[0]))\n",
    "\n",
    "        print(actions)\n",
    "        #print(hello)\n",
    "        #e-greedy policy\n",
    "        if random.random() > epsilon:\n",
    "            new_action=torch.argmax(actions)\n",
    "            \n",
    "        else:\n",
    "            new_action=torch.argmax(actions)\n",
    "            \n",
    "\n",
    "        epsilon = max(E_MIN, E_DECAY*epsilon)\n",
    "        new_states = env.step(new_action.item())\n",
    "\n",
    "        memory_buffer.append(experience(state,new_action,reward,new_state))\n",
    "        \n",
    "        print(new_states)\n",
    "\n",
    "        if (j + 1)%4 == 0:\n",
    "\n",
    "        print(val)        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "74c14147-8df7-4a26-bfe2-7043e8eded15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([3, 6])\n",
      "tensor([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# find the maximum value and index of the tensor along the second dimension\n",
    "max_val, max_idx = torch.max(x, dim=1)\n",
    "\n",
    "print(x)\n",
    "print(max_val)\n",
    "print(max_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c7811956-0d31-4a7d-a5b4-57156566ad6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "8\n",
      "12\n",
      "16\n",
      "20\n",
      "24\n",
      "28\n",
      "32\n",
      "36\n",
      "40\n",
      "44\n",
      "48\n",
      "52\n",
      "56\n",
      "60\n",
      "64\n",
      "68\n",
      "72\n",
      "76\n",
      "80\n",
      "84\n",
      "88\n",
      "92\n",
      "96\n",
      "100\n",
      "104\n",
      "108\n",
      "112\n",
      "116\n",
      "120\n",
      "124\n",
      "128\n",
      "132\n",
      "136\n",
      "140\n",
      "144\n",
      "148\n",
      "152\n",
      "156\n",
      "160\n",
      "164\n",
      "168\n",
      "172\n",
      "176\n",
      "180\n",
      "184\n",
      "188\n",
      "192\n",
      "196\n",
      "200\n",
      "204\n",
      "208\n",
      "212\n",
      "216\n",
      "220\n",
      "224\n",
      "228\n",
      "232\n",
      "236\n",
      "240\n",
      "244\n",
      "248\n",
      "252\n",
      "256\n",
      "260\n",
      "264\n",
      "268\n",
      "272\n",
      "276\n",
      "280\n",
      "284\n",
      "288\n",
      "292\n",
      "296\n",
      "300\n",
      "304\n",
      "308\n",
      "312\n",
      "316\n",
      "320\n",
      "324\n",
      "328\n",
      "332\n",
      "336\n",
      "340\n",
      "344\n",
      "348\n",
      "352\n",
      "356\n",
      "360\n",
      "364\n",
      "368\n",
      "372\n",
      "376\n",
      "380\n",
      "384\n",
      "388\n",
      "392\n",
      "396\n",
      "400\n",
      "404\n",
      "408\n",
      "412\n",
      "416\n",
      "420\n",
      "424\n",
      "428\n",
      "432\n",
      "436\n",
      "440\n",
      "444\n",
      "448\n",
      "452\n",
      "456\n",
      "460\n",
      "464\n",
      "468\n",
      "472\n",
      "476\n",
      "480\n",
      "484\n",
      "488\n",
      "492\n",
      "496\n",
      "500\n",
      "504\n",
      "508\n",
      "512\n",
      "516\n",
      "520\n",
      "524\n",
      "528\n",
      "532\n",
      "536\n",
      "540\n",
      "544\n",
      "548\n",
      "552\n",
      "556\n",
      "560\n",
      "564\n",
      "568\n",
      "572\n",
      "576\n",
      "580\n",
      "584\n",
      "588\n",
      "592\n",
      "596\n",
      "600\n",
      "604\n",
      "608\n",
      "612\n",
      "616\n",
      "620\n",
      "624\n",
      "628\n",
      "632\n",
      "636\n",
      "640\n",
      "644\n",
      "648\n",
      "652\n",
      "656\n",
      "660\n",
      "664\n",
      "668\n",
      "672\n",
      "676\n",
      "680\n",
      "684\n",
      "688\n",
      "692\n",
      "696\n",
      "700\n",
      "704\n",
      "708\n",
      "712\n",
      "716\n",
      "720\n",
      "724\n",
      "728\n",
      "732\n",
      "736\n",
      "740\n",
      "744\n",
      "748\n",
      "752\n",
      "756\n",
      "760\n",
      "764\n",
      "768\n",
      "772\n",
      "776\n",
      "780\n",
      "784\n",
      "788\n",
      "792\n",
      "796\n",
      "800\n",
      "804\n",
      "808\n",
      "812\n",
      "816\n",
      "820\n",
      "824\n",
      "828\n",
      "832\n",
      "836\n",
      "840\n",
      "844\n",
      "848\n",
      "852\n",
      "856\n",
      "860\n",
      "864\n",
      "868\n",
      "872\n",
      "876\n",
      "880\n",
      "884\n",
      "888\n",
      "892\n",
      "896\n",
      "900\n",
      "904\n",
      "908\n",
      "912\n",
      "916\n",
      "920\n",
      "924\n",
      "928\n",
      "932\n",
      "936\n",
      "940\n",
      "944\n",
      "948\n",
      "952\n",
      "956\n",
      "960\n",
      "964\n",
      "968\n",
      "972\n",
      "976\n",
      "980\n",
      "984\n",
      "988\n",
      "992\n",
      "996\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for i in range(max_num_timesteps):\n",
    "    counter = counter + 1\n",
    "    if (i + 1) % 4 == 0:\n",
    "        print(counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8336101-def3-48ef-8033-2a17f92d2ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
